{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `mlarena.utils.data_utils` Demo\n",
    "\n",
    "This notebook serves as a demonstration of the various data cleaning and manipulation utilities available in the `mlarena.utils.data_utils` module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlarena.utils.data_utils as dut\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Transform Data Columns\n",
    "\n",
    "It is common for a dataframe to have date columns stored as strings. This handy function `transform_date_cols` helps you transform them. \n",
    "\n",
    "- Flexible input handling: Works with either a single column or multiple columns\n",
    "- Format customization: Supports any date format using standard Python strftime directives\n",
    "    - %d: Day of the month as a zero-padded decimal (e.g., 25)\n",
    "    - %m: Month as a zero-padded decimal number (e.g., 08)\n",
    "    - %b: Abbreviated month name (e.g., Aug)\n",
    "    - %Y: Four-digit year (e.g., 2024)\n",
    "- Smart case handling: Automatically normalizes month abbreviations (like 'JAN', 'jan', 'Jan') when using %b format\n",
    "- Type safety: Preserves existing datetime columns without unnecessary conversion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date1         object\n",
      "date2         object\n",
      "date3         object\n",
      "date4         object\n",
      "not_a_date    object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Sample DataFrame with different date formats\n",
    "df_test = pd.DataFrame({\n",
    "    \"date1\": [\"20240101\", \"20240215\", \"20240320\"],\n",
    "    \"date2\": [\"25-08-2024\", \"15-09-2024\", \"01-10-2024\"],\n",
    "    \"date3\": [\"25Aug2024\", \"15AUG2024\", \"01aug2024\"],  # different cases\n",
    "    \"date4\": [\"20240801\", \"20240915\", \"20240311\"],\n",
    "    \"not_a_date\": [123, \"abc\", None]\n",
    "})\n",
    "print(df_test.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date1         datetime64[ns]\n",
      "date2         datetime64[ns]\n",
      "date3         datetime64[ns]\n",
      "date4         datetime64[ns]\n",
      "not_a_date            object\n",
      "dtype: object\n",
      "       date1      date2      date3      date4 not_a_date\n",
      "0 2024-01-01 2024-08-25 2024-08-25 2024-08-01        123\n",
      "1 2024-02-15 2024-09-15 2024-08-15 2024-09-15        abc\n",
      "2 2024-03-20 2024-10-01 2024-08-01 2024-03-11       None\n"
     ]
    }
   ],
   "source": [
    "# Apply the function \n",
    "df_result = dut.transform_date_cols(df_test, [\"date1\", \"date4\"], \"%Y%m%d\") # take a list\n",
    "df_result = dut.transform_date_cols(df_result, \"date2\", \"%d-%m-%Y\") # take one column\n",
    "df_result = dut.transform_date_cols(df_result, [\"date3\"], \"%d%b%Y\") # handle column with different cases automatically\n",
    "\n",
    "# Display result\n",
    "print(df_result.dtypes)\n",
    "print(df_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Clean Dollar Columns\n",
    "It is common for a dataframe to have dollar amount columns stored as strings with currency symbols and commas. The `clean_dollar_cols` function helps you transform these into numeric values.\n",
    "\n",
    "- Flexible input handling: Works with either a single column or multiple columns\n",
    "- Clean the column(s) off currency symbols and commas\n",
    "- Type conversion: Converts the cleaned strings to float values for numerical analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "       price  revenue other\n",
      "0  $1,234.56   12,000     A\n",
      "1  $2,345.67              B\n",
      "2  $3,456.78  $30,000     C\n",
      "\n",
      "Dtypes:\n",
      "price      object\n",
      "revenue    object\n",
      "other      object\n",
      "dtype: object\n",
      "\n",
      "Cleaned DataFrame:\n",
      "     price  revenue other\n",
      "0  1234.56  12000.0     A\n",
      "1  2345.67      NaN     B\n",
      "2  3456.78  30000.0     C\n",
      "\n",
      "Dtypes:\n",
      "price      float64\n",
      "revenue    float64\n",
      "other       object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "df_dollars = pd.DataFrame({\n",
    "    'price': ['$1,234.56', '$2,345.67', '$3,456.78'],\n",
    "    'revenue': ['12,000', '', '$30,000'],\n",
    "    'other': ['A', 'B', 'C']\n",
    "})\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "print(df_dollars)\n",
    "print(\"\\nDtypes:\")\n",
    "print(df_dollars.dtypes)\n",
    "\n",
    "df_cleaned = dut.clean_dollar_cols(df_dollars, ['price', 'revenue'])\n",
    "\n",
    "print(\"\\nCleaned DataFrame:\")\n",
    "print(df_cleaned)\n",
    "print(\"\\nDtypes:\")\n",
    "print(df_cleaned.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Value Counts with Percent\n",
    "The `value_counts_with_pct` function enhances pandas' built-in value_counts by adding percentage information alongside counts.\n",
    "\n",
    "- Comprehensive view: Shows both raw counts and percentages in a single output\n",
    "- Flexible NA handling: Option to include or exclude NA values from the analysis\n",
    "- Clear formatting: Percentages are formatted with a specified number of decimal places\n",
    "- Sorted results: Values are sorted by frequency for easy interpretation\n",
    "- Useful for: Quick categorical data profiling, understanding class distributions, and reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value counts for 'color' (including NA):\n",
      "   color  count    pct\n",
      "0    red      3  42.86\n",
      "1   blue      2  28.57\n",
      "2  green      1  14.29\n",
      "3   None      1  14.29\n",
      "\n",
      "Value counts for 'color' (excluding NA):\n",
      "   color  count    pct\n",
      "0    red      3  50.00\n",
      "1   blue      2  33.33\n",
      "2  green      1  16.67\n",
      "\n",
      "Value counts for ['color', 'size']:\n",
      "   color size  count    pct\n",
      "0    red    L      2  28.57\n",
      "1   blue    M      1  14.29\n",
      "2   blue    S      1  14.29\n",
      "3  green    M      1  14.29\n",
      "4    red    S      1  14.29\n",
      "5    NaN    M      1  14.29\n"
     ]
    }
   ],
   "source": [
    "df_categories = pd.DataFrame({\n",
    "    'color': ['red', 'blue', 'red', 'green', 'blue', 'red', None],\n",
    "    'size': ['S', 'M', 'L', 'M', 'S', 'L', 'M']\n",
    "})\n",
    "\n",
    "print(\"Value counts for 'color' (including NA):\")\n",
    "print(dut.value_counts_with_pct(df_categories, 'color'))\n",
    "\n",
    "print(\"\\nValue counts for 'color' (excluding NA):\")\n",
    "print(dut.value_counts_with_pct(df_categories, 'color', dropna=True))\n",
    "\n",
    "print(\"\\nValue counts for ['color', 'size']:\")\n",
    "print(dut.value_counts_with_pct(df_categories, ['color','size']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Drop Fully Null Columns\n",
    "The `drop_fully_null_cols` function is specifically designed to prevent issues with Databricks' `display()` function, which can break when encountering columns that are entirely null (as it cannot infer the schema).\n",
    "\n",
    "- Prevents Databricks display errors: Removes columns that would cause schema inference issues\n",
    "- Safe operation: Returns a new DataFrame without modifying the original\n",
    "- Common usage: `drop_fully_null_cols(df).display()` in Databricks notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "   col1  col2  col3  col4\n",
      "0     1   NaN     A  None\n",
      "1     2   NaN  None  None\n",
      "2     3   NaN     C  None\n",
      "üóëÔ∏è Dropped fully-null columns: ['col2', 'col4']\n",
      "\n",
      "Cleaned DataFrame:\n",
      "   col1  col3\n",
      "0     1     A\n",
      "1     2  None\n",
      "2     3     C\n"
     ]
    }
   ],
   "source": [
    "df_nulls = pd.DataFrame({\n",
    "    'col1': [1, 2, 3],\n",
    "    'col2': [np.nan, np.nan, np.nan],  # Fully null\n",
    "    'col3': ['A', None, 'C'],\n",
    "    'col4': [None, None, None]  # Fully null\n",
    "})\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "print(df_nulls)\n",
    "\n",
    "df_cleaned = dut.drop_fully_null_cols(df_nulls, verbose=True)\n",
    "\n",
    "print(\"\\nCleaned DataFrame:\")\n",
    "print(df_cleaned) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Print Schema Alphabetically\n",
    "The `print_schema_alphabetically` function is particularly useful when exploring very wide DataFrames with many columns. By sorting column names alphabetically, it makes it easier to:\n",
    "\n",
    "- Quickly locate specific columns in large datasets\n",
    "- Compare schemas between different DataFrames to identify missing or additional columns\n",
    "- Maintain a consistent view of your data structure regardless of the original column order\n",
    "- Simplify documentation and reporting of data structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "   z_price a_category     m_date  b_is_active  y_quantity\n",
      "0    100.5          A 2024-01-01         True           1\n",
      "1    200.5          B 2024-01-02        False           2\n",
      "2    300.5          C 2024-01-03         True           3\n",
      "\n",
      "Schema in alphabetical order:\n",
      "a_category             object\n",
      "b_is_active              bool\n",
      "m_date         datetime64[ns]\n",
      "y_quantity              int32\n",
      "z_price               float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "    'z_price': [100.5, 200.5, 300.5],\n",
    "    'a_category': ['A', 'B', 'C'],\n",
    "    'm_date': pd.date_range('2024-01-01', periods=3),\n",
    "    'b_is_active': [True, False, True],\n",
    "    'y_quantity': np.array([1, 2, 3], dtype='int32')\n",
    "})\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "print(\"\\nSchema in alphabetical order:\")\n",
    "dut.print_schema_alphabetically(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Check Primary Key\n",
    "The `is_primary_key` function helps verify if a column or combination of columns could serve as a primary key in a DataFrame.\n",
    "\n",
    "A traditional primary key must satisfy two key requirements:\n",
    "1. Uniqueness: Each combination of values must be unique across all rows\n",
    "2. No null values: Primary key columns cannot contain null/missing values\n",
    "\n",
    "However, in real-world data analysis, we often encounter datasets where potential key columns contain some missing values. This function takes a practical approach by:\n",
    "1. Alerting you about any missing values in the potential key columns\n",
    "2. Checking if the columns would form a unique identifier after removing rows with missing values\n",
    "\n",
    "This function is useful for:\n",
    "- Data quality assessment: Understanding the completeness and uniqueness of your key fields\n",
    "- Database schema design: Identifying potential primary keys even in imperfect data\n",
    "- ETL validation: Verifying key constraints while being aware of data quality issues\n",
    "- Data integrity checks: Ensuring uniqueness for joins/merges after handling missing values\n",
    "\n",
    "The function accepts either a single column name or a list of columns, making it flexible for checking both simple and composite keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test 1: Single column primary key\n",
      "‚úÖ There are no missing values in column 'id'.\n",
      "‚ÑπÔ∏è Total row count after filtering out missings: 5\n",
      "‚ÑπÔ∏è Unique row count after filtering out missings: 5\n",
      "üîë The column(s) 'id' form a primary key.\n",
      "\n",
      "Test 2: Column with duplicates\n",
      "‚úÖ There are no missing values in column 'category'.\n",
      "‚ÑπÔ∏è Total row count after filtering out missings: 5\n",
      "‚ÑπÔ∏è Unique row count after filtering out missings: 3\n",
      "‚ùå The column(s) 'category' do not form a primary key.\n",
      "\n",
      "Test 3: Composite primary key\n",
      "‚úÖ There are no missing values in columns 'category', 'date'.\n",
      "‚ÑπÔ∏è Total row count after filtering out missings: 5\n",
      "‚ÑπÔ∏è Unique row count after filtering out missings: 5\n",
      "üîë The column(s) 'category', 'date' form a primary key.\n",
      "\n",
      "Test 4: Column with null values\n",
      "‚ö†Ô∏è There are 1 row(s) with missing values in column 'code'.\n",
      "‚úÖ There are no missing values in column 'date'.\n",
      "‚ÑπÔ∏è Total row count after filtering out missings: 4\n",
      "‚ÑπÔ∏è Unique row count after filtering out missings: 4\n",
      "üîë The column(s) 'code', 'date' form a primary key after removing rows with missing values.\n",
      "\n",
      "Test 5: Empty DataFrame\n",
      "‚ùå DataFrame is empty.\n",
      "\n",
      "Test 6: Non-existent column\n",
      "‚ùå Column(s) 'not_a_column' do not exist in the DataFrame.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create sample DataFrame with different primary key scenarios\n",
    "df = pd.DataFrame({\n",
    "    # Single column primary key\n",
    "    'id': [1, 2, 3, 4, 5],\n",
    "    \n",
    "    # Column with duplicates\n",
    "    'category': ['A', 'B', 'A', 'B', 'C'],\n",
    "    \n",
    "    # Date column with some duplicates\n",
    "    'date': ['2024-01-01', '2024-01-01', '2024-01-02', '2024-01-02', '2024-01-03'],\n",
    "    \n",
    "    # Column with null values\n",
    "    'code': ['X1', None, 'X3', 'X4', 'X5'],\n",
    "    \n",
    "    # Values column\n",
    "    'value': [100, 200, 300, 400, 500]\n",
    "})\n",
    "\n",
    "# Test 1: Single column that is a primary key\n",
    "print(\"\\nTest 1: Single column primary key\")\n",
    "dut.is_primary_key(df, ['id'])  # Should return True\n",
    "\n",
    "# Test 2: Single column that is not a primary key (has duplicates)\n",
    "print(\"\\nTest 2: Column with duplicates\")\n",
    "dut.is_primary_key(df, ['category'])  # Should return False\n",
    "\n",
    "# Test 3: Multiple columns that together form a primary key\n",
    "print(\"\\nTest 3: Composite primary key\")\n",
    "dut.is_primary_key(df, ['category', 'date'])  # Should return True\n",
    "\n",
    "# Test 4: Column with null values\n",
    "print(\"\\nTest 4: Column with null values\")\n",
    "dut.is_primary_key(df, ['code','date'])  # Should return True\n",
    "\n",
    "# Test 5: Empty DataFrame\n",
    "print(\"\\nTest 5: Empty DataFrame\")\n",
    "empty_df = pd.DataFrame(columns=['id', 'value'])\n",
    "dut.is_primary_key(empty_df, ['id'])  # Should return False\n",
    "\n",
    "# Test 6: Non-existent column\n",
    "print(\"\\nTest 6: Non-existent column\")\n",
    "dut.is_primary_key(df, ['not_a_column'])  # Should return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Select Existing Columns\n",
    "The `select_existing_cols` function provides a safe way to select columns from a DataFrame, handling cases where some requested columns might not exist.\n",
    "\n",
    "- Safe column selection: Returns only columns that exist in the DataFrame\n",
    "- Case sensitivity options: Can match column names exactly or case-insensitively (default: case sensitive)\n",
    "- Verbose mode: Optional detailed output about which columns were found/missing (default: not verbose)\n",
    "- Useful for: Data pipeline robustness, handling dynamic column selections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "   A  B  C  Mixed_Case\n",
      "0  1  4  7          10\n",
      "1  2  5  8          11\n",
      "2  3  6  9          12\n",
      "\n",
      "Example 1: Select existing columns\n",
      "   A  C\n",
      "0  1  7\n",
      "1  2  8\n",
      "2  3  9\n",
      "\n",
      "Example 2: Case-insensitive matching\n",
      "‚úÖ Columns found: ['A', 'Mixed_Case']\n",
      "   A  Mixed_Case\n",
      "0  1          10\n",
      "1  2          11\n",
      "2  3          12\n",
      "\n",
      "Example 3: Verbose output with missing columns\n",
      "‚úÖ Columns found: ['A', 'B']\n",
      "‚ö†Ô∏è Columns not found: ['Missing1', 'Missing2']\n",
      "   A  B\n",
      "0  1  4\n",
      "1  2  5\n",
      "2  3  6\n"
     ]
    }
   ],
   "source": [
    "# Create sample DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'A': [1, 2, 3],\n",
    "    'B': [4, 5, 6],\n",
    "    'C': [7, 8, 9],\n",
    "    'Mixed_Case': [10, 11, 12]\n",
    "})\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# Example 1: Basic usage, case_sensitive, non-verbose\n",
    "print(\"\\nExample 1: Select existing columns\")\n",
    "result1 = dut.select_existing_cols(df, ['A', 'C', 'D'])\n",
    "print(result1)\n",
    "\n",
    "# Example 2: Case-insensitive matching\n",
    "print(\"\\nExample 2: Case-insensitive matching\")\n",
    "result2 = dut.select_existing_cols(df, ['a', 'mixed_case'], case_sensitive=False, verbose=True)\n",
    "print(result2)\n",
    "\n",
    "# Example 3: Verbose output\n",
    "print(\"\\nExample 3: Verbose output with missing columns\")\n",
    "result3 = dut.select_existing_cols(df, ['A', 'Missing1', 'B', 'Missing2'], verbose=True)\n",
    "print(result3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Find Duplicate Rows\n",
    "The `find_duplicates` function identifies and returns duplicate rows based on specified columns, making it easy to detect data quality issues or investigate duplicate records.\n",
    "\n",
    "Key features:\n",
    "- **Flexible column selection**: Works with single column or list of columns\n",
    "- **Comprehensive output**: Returns full duplicate rows with count information  \n",
    "- **NULL handling**: Automatically excludes rows with NULL values in key columns\n",
    "- **Smart column ordering**: Places count and key columns first, followed by other columns\n",
    "- **Useful for**: Data quality assessment, duplicate investigation, data cleaning validation\n",
    "\n",
    "Common use cases:\n",
    "- **Customer data**: Find duplicate customers based on email, phone, or name\n",
    "- **Transaction analysis**: Identify potential duplicate transactions  \n",
    "- **Data validation**: Check for unexpected duplicates before processing\n",
    "- **Business logic**: Find entities that appear multiple times in key dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original customer data:\n",
      "   customer_id              email     phone           name registration_date\n",
      "0            1    alice@email.com  555-0001    Alice Smith        2024-01-01\n",
      "1            2      bob@email.com  555-0002    Bob Johnson        2024-01-05\n",
      "2            3    alice@email.com  555-0003    Alice Brown        2024-01-10\n",
      "3            4  charlie@email.com  555-0004  Charlie Davis        2024-01-15\n",
      "4            5    david@email.com  555-0001   David Wilson        2024-01-20\n",
      "5            6      bob@email.com  555-0002    Bob Johnson        2024-01-25\n",
      "6            7               None  555-0007   Frank Miller        2024-01-30\n",
      "7            8      eve@email.com  555-0008     Eve Taylor        2024-02-01\n",
      "8            9    david@email.com  555-0001   David Wilson        2024-02-05\n",
      "\n",
      "Total customers: 9\n"
     ]
    }
   ],
   "source": [
    "# Create sample customer data with various duplicate scenarios\n",
    "customer_data = pd.DataFrame({\n",
    "    'customer_id': [1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
    "    'email': ['alice@email.com', 'bob@email.com', 'alice@email.com', 'charlie@email.com', \n",
    "              'david@email.com', 'bob@email.com', None, 'eve@email.com', 'david@email.com'],\n",
    "    'phone': ['555-0001', '555-0002', '555-0003', '555-0004', \n",
    "              '555-0001', '555-0002', '555-0007', '555-0008', '555-0001'],\n",
    "    'name': ['Alice Smith', 'Bob Johnson', 'Alice Brown', 'Charlie Davis',\n",
    "             'David Wilson', 'Bob Johnson', 'Frank Miller', 'Eve Taylor', 'David Wilson'],\n",
    "    'registration_date': ['2024-01-01', '2024-01-05', '2024-01-10', '2024-01-15',\n",
    "                         '2024-01-20', '2024-01-25', '2024-01-30', '2024-02-01', '2024-02-05']\n",
    "})\n",
    "\n",
    "print(\"Original customer data:\")\n",
    "print(customer_data)\n",
    "print(f\"\\nTotal customers: {len(customer_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1: Customers with duplicate emails\n",
      "   count            email  customer_id     phone          name  \\\n",
      "0      2  alice@email.com            1  555-0001   Alice Smith   \n",
      "1      2  alice@email.com            3  555-0003   Alice Brown   \n",
      "2      2    bob@email.com            2  555-0002   Bob Johnson   \n",
      "3      2    bob@email.com            6  555-0002   Bob Johnson   \n",
      "4      2  david@email.com            5  555-0001  David Wilson   \n",
      "5      2  david@email.com            9  555-0001  David Wilson   \n",
      "\n",
      "  registration_date  \n",
      "0        2024-01-01  \n",
      "1        2024-01-10  \n",
      "2        2024-01-05  \n",
      "3        2024-01-25  \n",
      "4        2024-01-20  \n",
      "5        2024-02-05  \n",
      "\n",
      "Found 6 rows with duplicate emails\n",
      "\n",
      "============================================================\n",
      "Example 2: Customers with duplicate phone numbers\n",
      "   count     phone  customer_id            email          name  \\\n",
      "0      3  555-0001            1  alice@email.com   Alice Smith   \n",
      "1      3  555-0001            5  david@email.com  David Wilson   \n",
      "2      3  555-0001            9  david@email.com  David Wilson   \n",
      "3      2  555-0002            2    bob@email.com   Bob Johnson   \n",
      "4      2  555-0002            6    bob@email.com   Bob Johnson   \n",
      "\n",
      "  registration_date  \n",
      "0        2024-01-01  \n",
      "1        2024-01-20  \n",
      "2        2024-02-05  \n",
      "3        2024-01-05  \n",
      "4        2024-01-25  \n",
      "\n",
      "Found 5 rows with duplicate phone numbers\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Find customers with duplicate emails\n",
    "print(\"Example 1: Customers with duplicate emails\")\n",
    "email_duplicates = dut.find_duplicates(customer_data, ['email'])\n",
    "print(email_duplicates)\n",
    "print(f\"\\nFound {len(email_duplicates)} rows with duplicate emails\")\n",
    "\n",
    "# Example 2: Find customers with duplicate phone numbers  \n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Example 2: Customers with duplicate phone numbers\")\n",
    "phone_duplicates = dut.find_duplicates(customer_data, ['phone'])\n",
    "print(phone_duplicates)\n",
    "print(f\"\\nFound {len(phone_duplicates)} rows with duplicate phone numbers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 3: Customers with duplicate name AND email combinations\n",
      "   count          name            email  customer_id     phone  \\\n",
      "0      2   Bob Johnson    bob@email.com            2  555-0002   \n",
      "1      2   Bob Johnson    bob@email.com            6  555-0002   \n",
      "2      2  David Wilson  david@email.com            5  555-0001   \n",
      "3      2  David Wilson  david@email.com            9  555-0001   \n",
      "\n",
      "  registration_date  \n",
      "0        2024-01-05  \n",
      "1        2024-01-25  \n",
      "2        2024-01-20  \n",
      "3        2024-02-05  \n",
      "\n",
      "Found 4 rows with duplicate name+email combinations\n",
      "\n",
      "============================================================\n",
      "Example 4: Comprehensive duplicate customer analysis\n",
      "Finding customers who might be the same person using multiple criteria...\n",
      "\n",
      "Phone numbers used with multiple emails: ['555-0001']\n",
      "Detailed view of customers with same phone but different emails:\n",
      "   customer_id     phone            email          name\n",
      "0            1  555-0001  alice@email.com   Alice Smith\n",
      "4            5  555-0001  david@email.com  David Wilson\n",
      "8            9  555-0001  david@email.com  David Wilson\n"
     ]
    }
   ],
   "source": [
    "# Example 3: Find customers with duplicate names AND emails (composite key)\n",
    "print(\"Example 3: Customers with duplicate name AND email combinations\")\n",
    "name_email_duplicates = dut.find_duplicates(customer_data, ['name', 'email'])\n",
    "print(name_email_duplicates)\n",
    "print(f\"\\nFound {len(name_email_duplicates)} rows with duplicate name+email combinations\")\n",
    "\n",
    "# Example 4: Business scenario - detecting potential duplicate customers\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Example 4: Comprehensive duplicate customer analysis\")\n",
    "print(\"Finding customers who might be the same person using multiple criteria...\")\n",
    "\n",
    "# Check for same person with different emails but same phone\n",
    "same_phone_diff_email = customer_data.groupby('phone')['email'].nunique()\n",
    "suspicious_phones = same_phone_diff_email[same_phone_diff_email > 1].index\n",
    "print(f\"\\nPhone numbers used with multiple emails: {list(suspicious_phones)}\")\n",
    "\n",
    "if len(suspicious_phones) > 0:\n",
    "    print(\"Detailed view of customers with same phone but different emails:\")\n",
    "    suspicious_data = customer_data[customer_data['phone'].isin(suspicious_phones)]\n",
    "    print(suspicious_data[['customer_id', 'phone', 'email', 'name']].sort_values('phone'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 5: Edge cases and NULL handling\n",
      "Data with NULL values:\n",
      "   id category  value\n",
      "0   1        A    100\n",
      "1   2        B    200\n",
      "2   3     None    300\n",
      "3   4        A    100\n",
      "4   5     None    400\n",
      "\n",
      "Finding duplicates in 'category' column (NULLs excluded):\n",
      "   count category  id  value\n",
      "0      2        A   1    100\n",
      "1      2        A   4    100\n",
      "\n",
      "Finding duplicates in 'value' column:\n",
      "   count  value  id category\n",
      "0      2    100   1        A\n",
      "1      2    100   4        A\n",
      "\n",
      "Testing with data that has no duplicates:\n",
      "Empty DataFrame\n",
      "Columns: [count, name, id]\n",
      "Index: []\n",
      "Shape: (0, 3) - Returns empty DataFrame with proper column structure\n"
     ]
    }
   ],
   "source": [
    "# Example 5: Handling edge cases\n",
    "print(\"Example 5: Edge cases and NULL handling\")\n",
    "\n",
    "# Create data with edge cases\n",
    "edge_case_data = pd.DataFrame({\n",
    "    'id': [1, 2, 3, 4, 5],\n",
    "    'category': ['A', 'B', None, 'A', None],  # Has NULLs\n",
    "    'value': [100, 200, 300, 100, 400]\n",
    "})\n",
    "\n",
    "print(\"Data with NULL values:\")\n",
    "print(edge_case_data)\n",
    "\n",
    "# Find duplicates - NULLs will be excluded\n",
    "print(\"\\nFinding duplicates in 'category' column (NULLs excluded):\")\n",
    "category_dups = dut.find_duplicates(edge_case_data, ['category'])\n",
    "print(category_dups)\n",
    "\n",
    "# Find duplicates in 'value' column\n",
    "print(\"\\nFinding duplicates in 'value' column:\")\n",
    "value_dups = dut.find_duplicates(edge_case_data, ['value'])\n",
    "print(value_dups)\n",
    "\n",
    "# Test with no duplicates\n",
    "unique_data = pd.DataFrame({\n",
    "    'id': [1, 2, 3],\n",
    "    'name': ['Alice', 'Bob', 'Charlie']\n",
    "})\n",
    "\n",
    "print(\"\\nTesting with data that has no duplicates:\")\n",
    "no_dups = dut.find_duplicates(unique_data, ['name'])\n",
    "print(no_dups)\n",
    "print(f\"Shape: {no_dups.shape} - Returns empty DataFrame with proper column structure\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Smart Deduplication by Ranking\n",
    "The `deduplicate_by_rank` function provides intelligent duplicate removal by keeping the \"best\" record from each group based on a ranking criterion. This is the perfect complement to `find_duplicates` - first investigate, then intelligently resolve.\n",
    "\n",
    "Key features:\n",
    "- **Intelligent selection**: Keeps the best-ranked record per group instead of arbitrary duplicate removal\n",
    "- **Flexible ranking**: Ascending or descending order based on any column (date, score, priority, etc.)\n",
    "- **Advanced tiebreaking**: Prefer records with non-missing values in specified columns\n",
    "- **Multi-column grouping**: Support for composite keys (customer_id + product_id)\n",
    "- **Business-focused**: Designed for real-world scenarios like customer data cleanup\n",
    "\n",
    "This function is essential for:\n",
    "- **Customer master data**: Keep the most recent or complete customer record\n",
    "- **Transaction deduplication**: Keep highest value or most recent transaction\n",
    "- **Product catalog cleanup**: Keep most complete product information\n",
    "- **Data quality improvement**: Systematic duplicate resolution with business logic\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original customer records with duplicates:\n",
      "  customer_id record_date  profile_completeness               email     phone  \\\n",
      "0        C001  2024-01-01                   0.3       old@email.com      None   \n",
      "1        C001  2024-01-15                   0.9       new@email.com  555-0001   \n",
      "2        C001  2024-01-10                   0.6      temp@email.com  555-0002   \n",
      "3        C002  2024-01-05                   0.4                None      None   \n",
      "4        C002  2024-01-20                   0.8  customer@email.com  555-0003   \n",
      "5        C003  2024-01-25                   0.7      test@email.com  555-0004   \n",
      "6        C003  2024-01-30                   0.5                None  555-0005   \n",
      "7        C003  2024-01-28                   0.9  verified@email.com  555-0006   \n",
      "\n",
      "   total_orders  last_purchase_amount  \n",
      "0             5                 150.0  \n",
      "1            12                 320.5  \n",
      "2             8                 200.0  \n",
      "3             2                  75.0  \n",
      "4            15                 450.0  \n",
      "5            20                 180.0  \n",
      "6            18                 220.0  \n",
      "7            25                 380.0  \n",
      "\n",
      "Total records: 8\n",
      "Unique customers: 3\n",
      "\n",
      "Using find_duplicates to investigate:\n",
      "   count customer_id record_date  profile_completeness               email\n",
      "0      3        C001  2024-01-01                   0.3       old@email.com\n",
      "1      3        C001  2024-01-15                   0.9       new@email.com\n",
      "2      3        C001  2024-01-10                   0.6      temp@email.com\n",
      "3      2        C002  2024-01-05                   0.4                None\n",
      "4      2        C002  2024-01-20                   0.8  customer@email.com\n",
      "5      3        C003  2024-01-25                   0.7      test@email.com\n",
      "6      3        C003  2024-01-30                   0.5                None\n",
      "7      3        C003  2024-01-28                   0.9  verified@email.com\n"
     ]
    }
   ],
   "source": [
    "# Create sample customer data with duplicates for deduplication demo\n",
    "customer_records = pd.DataFrame({\n",
    "    'customer_id': ['C001', 'C001', 'C001', 'C002', 'C002', 'C003', 'C003', 'C003'],\n",
    "    'record_date': ['2024-01-01', '2024-01-15', '2024-01-10', '2024-01-05', '2024-01-20', \n",
    "                   '2024-01-25', '2024-01-30', '2024-01-28'],\n",
    "    'profile_completeness': [0.3, 0.9, 0.6, 0.4, 0.8, 0.7, 0.5, 0.9],\n",
    "    'email': ['old@email.com', 'new@email.com', 'temp@email.com', None, 'customer@email.com', \n",
    "              'test@email.com', None, 'verified@email.com'],\n",
    "    'phone': [None, '555-0001', '555-0002', None, '555-0003', '555-0004', '555-0005', '555-0006'],\n",
    "    'total_orders': [5, 12, 8, 2, 15, 20, 18, 25],\n",
    "    'last_purchase_amount': [150.00, 320.50, 200.00, 75.00, 450.00, 180.00, 220.00, 380.00]\n",
    "})\n",
    "\n",
    "print(\"Original customer records with duplicates:\")\n",
    "print(customer_records)\n",
    "print(f\"\\nTotal records: {len(customer_records)}\")\n",
    "print(f\"Unique customers: {customer_records['customer_id'].nunique()}\")\n",
    "\n",
    "# First, let's see what duplicates we have\n",
    "print(\"\\nUsing find_duplicates to investigate:\")\n",
    "duplicates = dut.find_duplicates(customer_records, ['customer_id'])\n",
    "print(duplicates[['count', 'customer_id', 'record_date', 'profile_completeness', 'email']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strategy 1: Keep most recent record per customer\n",
      "üîÑ Deduplicating 8 rows by ['customer_id']\n",
      "‚ÑπÔ∏è Found 3 unique groups\n",
      "‚úÖ Removed 5 duplicate rows\n",
      "üìä Final dataset: 3 rows\n",
      "\n",
      "Most recent records:\n",
      "  customer_id record_date  profile_completeness               email  \\\n",
      "0        C001  2024-01-15                   0.9       new@email.com   \n",
      "1        C002  2024-01-20                   0.8  customer@email.com   \n",
      "2        C003  2024-01-30                   0.5                None   \n",
      "\n",
      "   total_orders  \n",
      "0            12  \n",
      "1            15  \n",
      "2            18  \n",
      "\n",
      "======================================================================\n",
      "Strategy 2: Keep record with highest profile completeness\n",
      "üîÑ Deduplicating 8 rows by ['customer_id']\n",
      "‚ÑπÔ∏è Found 3 unique groups\n",
      "‚úÖ Removed 5 duplicate rows\n",
      "üìä Final dataset: 3 rows\n",
      "\n",
      "Most complete profiles:\n",
      "  customer_id record_date  profile_completeness               email  \\\n",
      "0        C001  2024-01-15                   0.9       new@email.com   \n",
      "1        C002  2024-01-20                   0.8  customer@email.com   \n",
      "2        C003  2024-01-28                   0.9  verified@email.com   \n",
      "\n",
      "   total_orders  \n",
      "0            12  \n",
      "1            15  \n",
      "2            25  \n"
     ]
    }
   ],
   "source": [
    "# Strategy 1: Keep most recent record per customer\n",
    "print(\"Strategy 1: Keep most recent record per customer\")\n",
    "most_recent = dut.deduplicate_by_rank(\n",
    "    customer_records, \n",
    "    id_cols='customer_id', \n",
    "    ranking_col='record_date', \n",
    "    ascending=False,  # False = keep latest date\n",
    "    verbose=True\n",
    ")\n",
    "print(\"\\nMost recent records:\")\n",
    "print(most_recent[['customer_id', 'record_date', 'profile_completeness', 'email', 'total_orders']])\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# Strategy 2: Keep record with highest profile completeness\n",
    "print(\"Strategy 2: Keep record with highest profile completeness\")\n",
    "most_complete = dut.deduplicate_by_rank(\n",
    "    customer_records,\n",
    "    id_cols='customer_id',\n",
    "    ranking_col='profile_completeness',\n",
    "    ascending=False,  # False = keep highest completeness\n",
    "    verbose=True\n",
    ")\n",
    "print(\"\\nMost complete profiles:\")\n",
    "print(most_complete[['customer_id', 'record_date', 'profile_completeness', 'email', 'total_orders']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strategy 3: Most recent record with email preference as tiebreaker\n",
      "Data with tied dates:\n",
      "  customer_id record_date  profile_completeness               email     phone\n",
      "0        C004  2024-01-15                   0.7                None  555-0007\n",
      "1        C004  2024-01-15                   0.8  customer@email.com  555-0008\n",
      "2        C005  2024-01-20                   0.6      user@email.com  555-0009\n",
      "3        C005  2024-01-20                   0.9                None  555-0010\n",
      "\n",
      "Without tiebreaker (might select any tied record):\n",
      "  customer_id record_date  profile_completeness           email     phone\n",
      "0        C004  2024-01-15                   0.7            None  555-0007\n",
      "1        C005  2024-01-20                   0.6  user@email.com  555-0009\n",
      "\n",
      "With email tiebreaker (prefers non-null email):\n",
      "  customer_id record_date  profile_completeness               email     phone\n",
      "0        C004  2024-01-15                   0.8  customer@email.com  555-0008\n",
      "1        C005  2024-01-20                   0.6      user@email.com  555-0009\n"
     ]
    }
   ],
   "source": [
    "# Strategy 3: Advanced - Keep most recent record but break ties by preferring records with email\n",
    "print(\"Strategy 3: Most recent record with email preference as tiebreaker\")\n",
    "\n",
    "# Create data where we have ties in dates to demonstrate tiebreaker\n",
    "tied_data = pd.DataFrame({\n",
    "    'customer_id': ['C004', 'C004', 'C005', 'C005'],\n",
    "    'record_date': ['2024-01-15', '2024-01-15', '2024-01-20', '2024-01-20'],  # Same dates\n",
    "    'profile_completeness': [0.7, 0.8, 0.6, 0.9],\n",
    "    'email': [None, 'customer@email.com', 'user@email.com', None],  # One has email, one doesn't\n",
    "    'phone': ['555-0007', '555-0008', '555-0009', '555-0010']\n",
    "})\n",
    "\n",
    "print(\"Data with tied dates:\")\n",
    "print(tied_data)\n",
    "\n",
    "# Without tiebreaker - arbitrary selection\n",
    "without_tiebreaker = dut.deduplicate_by_rank(\n",
    "    tied_data,\n",
    "    id_cols='customer_id',\n",
    "    ranking_col='record_date',\n",
    "    ascending=False\n",
    ")\n",
    "print(\"\\nWithout tiebreaker (might select any tied record):\")\n",
    "print(without_tiebreaker)\n",
    "\n",
    "# With tiebreaker - prefer non-null email  \n",
    "with_tiebreaker = dut.deduplicate_by_rank(\n",
    "    tied_data,\n",
    "    id_cols='customer_id', \n",
    "    ranking_col='record_date',\n",
    "    ascending=False,\n",
    "    tiebreaker_col='email'  # Prefer records with non-null email\n",
    ")\n",
    "print(\"\\nWith email tiebreaker (prefers non-null email):\")\n",
    "print(with_tiebreaker)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Business Use Case: Transaction Deduplication\n",
      "Transaction data with potential duplicates:\n",
      "  customer_id product_id transaction_date  amount payment_method  \\\n",
      "0        C001       P001       2024-01-01   100.0         Credit   \n",
      "1        C001       P001       2024-01-01   150.0         Credit   \n",
      "2        C001       P002       2024-01-02   200.0          Debit   \n",
      "3        C002       P001       2024-01-03    75.0           Cash   \n",
      "4        C002       P001       2024-01-04   125.0         Credit   \n",
      "5        C003       P002       2024-01-05   180.0         Credit   \n",
      "6        C003       P002       2024-01-05   220.0          Debit   \n",
      "\n",
      "  store_location  \n",
      "0        Store A  \n",
      "1        Store A  \n",
      "2        Store B  \n",
      "3        Store A  \n",
      "4        Store B  \n",
      "5        Store A  \n",
      "6        Store A  \n",
      "\n",
      "Finding duplicate customer-product combinations:\n",
      "   count customer_id product_id transaction_date  amount payment_method  \\\n",
      "0      2        C001       P001       2024-01-01   100.0         Credit   \n",
      "1      2        C001       P001       2024-01-01   150.0         Credit   \n",
      "2      2        C002       P001       2024-01-03    75.0           Cash   \n",
      "3      2        C002       P001       2024-01-04   125.0         Credit   \n",
      "4      2        C003       P002       2024-01-05   180.0         Credit   \n",
      "5      2        C003       P002       2024-01-05   220.0          Debit   \n",
      "\n",
      "  store_location  \n",
      "0        Store A  \n",
      "1        Store A  \n",
      "2        Store A  \n",
      "3        Store B  \n",
      "4        Store A  \n",
      "5        Store A  \n",
      "\n",
      "Deduplication Strategy: Keep highest amount per customer-product\n",
      "üîÑ Deduplicating 7 rows by ['customer_id', 'product_id']\n",
      "‚ÑπÔ∏è Found 4 unique groups\n",
      "‚úÖ Removed 3 duplicate rows\n",
      "üìä Final dataset: 4 rows\n",
      "\n",
      "Final deduplicated transactions:\n",
      "  customer_id product_id transaction_date  amount payment_method  \\\n",
      "0        C001       P001       2024-01-01   150.0         Credit   \n",
      "1        C001       P002       2024-01-02   200.0          Debit   \n",
      "2        C002       P001       2024-01-04   125.0         Credit   \n",
      "3        C003       P002       2024-01-05   220.0          Debit   \n",
      "\n",
      "  store_location  \n",
      "0        Store A  \n",
      "1        Store B  \n",
      "2        Store B  \n",
      "3        Store A  \n",
      "\n",
      "Summary:\n",
      "Original transactions: 7\n",
      "After deduplication: 4\n",
      "Removed duplicates: 3\n"
     ]
    }
   ],
   "source": [
    "# Business Use Case: Transaction Analysis\n",
    "print(\"Business Use Case: Transaction Deduplication\")\n",
    "\n",
    "# Create transaction data with potential duplicates\n",
    "transactions = pd.DataFrame({\n",
    "    'customer_id': ['C001', 'C001', 'C001', 'C002', 'C002', 'C003', 'C003'],\n",
    "    'product_id': ['P001', 'P001', 'P002', 'P001', 'P001', 'P002', 'P002'],\n",
    "    'transaction_date': ['2024-01-01', '2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', \n",
    "                        '2024-01-05', '2024-01-05'],\n",
    "    'amount': [100.00, 150.00, 200.00, 75.00, 125.00, 180.00, 220.00],\n",
    "    'payment_method': ['Credit', 'Credit', 'Debit', 'Cash', 'Credit', 'Credit', 'Debit'],\n",
    "    'store_location': ['Store A', 'Store A', 'Store B', 'Store A', 'Store B', 'Store A', 'Store A']\n",
    "})\n",
    "\n",
    "print(\"Transaction data with potential duplicates:\")\n",
    "print(transactions)\n",
    "\n",
    "# Find duplicates first\n",
    "print(\"\\nFinding duplicate customer-product combinations:\")\n",
    "transaction_dups = dut.find_duplicates(transactions, ['customer_id', 'product_id'])\n",
    "print(transaction_dups)\n",
    "\n",
    "# Strategy: Keep highest amount transaction per customer-product combination\n",
    "print(\"\\nDeduplication Strategy: Keep highest amount per customer-product\")\n",
    "deduplicated_transactions = dut.deduplicate_by_rank(\n",
    "    transactions,\n",
    "    id_cols=['customer_id', 'product_id'],\n",
    "    ranking_col='amount',\n",
    "    ascending=False,  # Keep highest amount\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\nFinal deduplicated transactions:\")\n",
    "print(deduplicated_transactions)\n",
    "\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"Original transactions: {len(transactions)}\")\n",
    "print(f\"After deduplication: {len(deduplicated_transactions)}\")\n",
    "print(f\"Removed duplicates: {len(transactions) - len(deduplicated_transactions)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1 Complete Duplicate Management Workflow\n",
    "\n",
    "Together, the three functions below provide a systematic \"Discover ‚Üí Investigate ‚Üí Resolve\" approach:\n",
    "\n",
    "- Use `is_primary_key` to discover the existance of duplication issues\n",
    "- Use `find_duplicates` to analyze duplicate patterns\n",
    "- Use `deduplicate_by_rank` to intelligently resolve duplicates with business logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Customer Data:\n",
      "  customer_id                    email     phone registration_date  \\\n",
      "0        C001          alice@email.com  555-0001        2024-01-01   \n",
      "1        C002            bob@email.com  555-0002        2024-01-15   \n",
      "2        C003        charlie@email.com  555-0003        2024-02-01   \n",
      "3        C001  alice.updated@email.com  555-0001        2024-02-15   \n",
      "4        C004          diana@email.com  555-0004        2024-03-01   \n",
      "5        C002            bob@email.com  555-0005        2024-03-15   \n",
      "6        C005            eve@email.com  555-0006        2024-04-01   \n",
      "7        C003    charlie.new@email.com  555-0003        2024-04-15   \n",
      "8        C006          frank@email.com  555-0007        2024-05-01   \n",
      "\n",
      "   profile_completeness  total_orders  last_purchase_amount account_status  \n",
      "0                   0.6             5                150.50         Active  \n",
      "1                   0.8            12                320.00        Premium  \n",
      "2                   0.4             3                 75.25          Basic  \n",
      "3                   0.9             8                280.75        Premium  \n",
      "4                   0.7            15                450.00        Premium  \n",
      "5                   0.8            12                320.00        Premium  \n",
      "6                   0.5             2                 95.50          Basic  \n",
      "7                   0.8             7                180.00         Active  \n",
      "8                   0.6            10                220.00         Active  \n",
      "\n",
      "Dataset Info: 9 rows, 8 columns\n"
     ]
    }
   ],
   "source": [
    "# Sample customer data with intentional quality issues\n",
    "customer_data = pd.DataFrame({\n",
    "    'customer_id': ['C001', 'C002', 'C003', 'C001', 'C004', 'C002', 'C005', 'C003', 'C006'],\n",
    "    'email': ['alice@email.com', 'bob@email.com', 'charlie@email.com', 'alice.updated@email.com', \n",
    "              'diana@email.com', 'bob@email.com', 'eve@email.com', 'charlie.new@email.com', 'frank@email.com'],\n",
    "    'phone': ['555-0001', '555-0002', '555-0003', '555-0001', '555-0004', \n",
    "              '555-0005', '555-0006', '555-0003', '555-0007'],\n",
    "    'registration_date': ['2024-01-01', '2024-01-15', '2024-02-01', '2024-02-15', '2024-03-01',\n",
    "                         '2024-03-15', '2024-04-01', '2024-04-15', '2024-05-01'],\n",
    "    'profile_completeness': [0.6, 0.8, 0.4, 0.9, 0.7, 0.8, 0.5, 0.8, 0.6],\n",
    "    'total_orders': [5, 12, 3, 8, 15, 12, 2, 7, 10],\n",
    "    'last_purchase_amount': [150.50, 320.00, 75.25, 280.75, 450.00, 320.00, 95.50, 180.00, 220.00],\n",
    "    'account_status': ['Active', 'Premium', 'Basic', 'Premium', 'Premium', 'Premium', 'Basic', 'Active', 'Active']\n",
    "})\n",
    "\n",
    "print(\"Original Customer Data:\")\n",
    "print(customer_data)\n",
    "print(f\"\\nDataset Info: {len(customer_data)} rows, {customer_data.shape[1]} columns\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 1: DISCOVER - Check for Primary Key Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Checking if 'customer_id' can serve as primary key...\n",
      "‚úÖ There are no missing values in column 'customer_id'.\n",
      "‚ÑπÔ∏è Total row count after filtering out missings: 9\n",
      "‚ÑπÔ∏è Unique row count after filtering out missings: 6\n",
      "‚ùå The column(s) 'customer_id' do not form a primary key.\n",
      "\n",
      "‚ö†Ô∏è  PRIMARY KEY ISSUE DETECTED!\n",
      "   Multiple records exist for the same customer_id\n",
      "   ‚Üí Need to investigate and resolve duplicates\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check if customer_id is a proper primary key\n",
    "print(\"\\nüîç Checking if 'customer_id' can serve as primary key...\")\n",
    "is_unique = dut.is_primary_key(customer_data, 'customer_id', verbose=True)\n",
    "\n",
    "if not is_unique:\n",
    "    print(\"\\n‚ö†Ô∏è  PRIMARY KEY ISSUE DETECTED!\")\n",
    "    print(\"   Multiple records exist for the same customer_id\")\n",
    "    print(\"   ‚Üí Need to investigate and resolve duplicates\\n\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ No primary key issues detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 2: INVESTIGATE - Analyze Duplicate Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Finding duplicate customer records...\n",
      "\n",
      "üìä DUPLICATE ANALYSIS RESULTS:\n",
      "   ‚Ä¢ Found 6 duplicate rows\n",
      "   ‚Ä¢ Affecting 3 customers\n",
      "\n",
      "üìã Detailed duplicate records:\n",
      "   count customer_id                    email registration_date  \\\n",
      "0      2        C001          alice@email.com        2024-01-01   \n",
      "1      2        C001  alice.updated@email.com        2024-02-15   \n",
      "2      2        C002            bob@email.com        2024-01-15   \n",
      "3      2        C002            bob@email.com        2024-03-15   \n",
      "4      2        C003        charlie@email.com        2024-02-01   \n",
      "5      2        C003    charlie.new@email.com        2024-04-15   \n",
      "\n",
      "   profile_completeness  total_orders  \n",
      "0                   0.6             5  \n",
      "1                   0.9             8  \n",
      "2                   0.8            12  \n",
      "3                   0.8            12  \n",
      "4                   0.4             3  \n",
      "5                   0.8             7  \n",
      "\n",
      "üîç DUPLICATE PATTERNS:\n",
      "\n",
      "   Customer C001:\n",
      "   ‚Ä¢ 2 duplicate records\n",
      "   ‚Ä¢ Registration dates: ['2024-01-01', '2024-02-15']\n",
      "   ‚Ä¢ Profile completeness: [0.6, 0.9]\n",
      "   ‚Ä¢ Email addresses: ['alice@email.com', 'alice.updated@email.com']\n",
      "\n",
      "   Customer C002:\n",
      "   ‚Ä¢ 2 duplicate records\n",
      "   ‚Ä¢ Registration dates: ['2024-01-15', '2024-03-15']\n",
      "   ‚Ä¢ Profile completeness: [0.8, 0.8]\n",
      "   ‚Ä¢ Email addresses: ['bob@email.com', 'bob@email.com']\n",
      "\n",
      "   Customer C003:\n",
      "   ‚Ä¢ 2 duplicate records\n",
      "   ‚Ä¢ Registration dates: ['2024-02-01', '2024-04-15']\n",
      "   ‚Ä¢ Profile completeness: [0.4, 0.8]\n",
      "   ‚Ä¢ Email addresses: ['charlie@email.com', 'charlie.new@email.com']\n"
     ]
    }
   ],
   "source": [
    "# Find and analyze all duplicates\n",
    "print(\"\\nüîç Finding duplicate customer records...\")\n",
    "duplicates = dut.find_duplicates(customer_data, ['customer_id'])\n",
    "\n",
    "if len(duplicates) > 0:\n",
    "    print(f\"\\nüìä DUPLICATE ANALYSIS RESULTS:\")\n",
    "    print(f\"   ‚Ä¢ Found {len(duplicates)} duplicate rows\")\n",
    "    print(f\"   ‚Ä¢ Affecting {duplicates['customer_id'].nunique()} customers\")\n",
    "    \n",
    "    print(f\"\\nüìã Detailed duplicate records:\")\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.width', None)\n",
    "    print(duplicates[['count', 'customer_id', 'email', 'registration_date', 'profile_completeness', 'total_orders']])\n",
    "    \n",
    "    # Analyze patterns\n",
    "    print(f\"\\nüîç DUPLICATE PATTERNS:\")\n",
    "    duplicate_customers = duplicates['customer_id'].unique()\n",
    "    for customer in duplicate_customers:\n",
    "        customer_records = duplicates[duplicates['customer_id'] == customer]\n",
    "        print(f\"\\n   Customer {customer}:\")\n",
    "        print(f\"   ‚Ä¢ {len(customer_records)} duplicate records\")\n",
    "        print(f\"   ‚Ä¢ Registration dates: {customer_records['registration_date'].tolist()}\")\n",
    "        print(f\"   ‚Ä¢ Profile completeness: {customer_records['profile_completeness'].tolist()}\")\n",
    "        print(f\"   ‚Ä¢ Email addresses: {customer_records['email'].tolist()}\")\n",
    "else:\n",
    "    print(\"‚úÖ No duplicates found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3: RESOLVE - Intelligently Remove Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ DEDUPLICATION STRATEGY:\n",
      "   ‚Ä¢ Keep most recent registration (latest date)\n",
      "   ‚Ä¢ Break ties by preferring higher profile completeness\n",
      "   ‚Ä¢ Ensure one record per customer\n",
      "üîÑ Deduplicating 9 rows by ['customer_id']\n",
      "‚ÑπÔ∏è Found 6 unique groups\n",
      "‚úÖ Removed 3 duplicate rows\n",
      "üìä Final dataset: 6 rows\n",
      "\n",
      "üìä DEDUPLICATION RESULTS:\n",
      "   ‚Ä¢ Original records: 9\n",
      "   ‚Ä¢ After deduplication: 6\n",
      "   ‚Ä¢ Records removed: 3\n",
      "\n",
      "‚úÖ CLEANED CUSTOMER DATA:\n",
      "  customer_id                    email registration_date  \\\n",
      "0        C001  alice.updated@email.com        2024-02-15   \n",
      "1        C002            bob@email.com        2024-03-15   \n",
      "2        C003    charlie.new@email.com        2024-04-15   \n",
      "3        C004          diana@email.com        2024-03-01   \n",
      "4        C005            eve@email.com        2024-04-01   \n",
      "5        C006          frank@email.com        2024-05-01   \n",
      "\n",
      "   profile_completeness  total_orders account_status  \n",
      "0                   0.9             8        Premium  \n",
      "1                   0.8            12        Premium  \n",
      "2                   0.8             7         Active  \n",
      "3                   0.7            15        Premium  \n",
      "4                   0.5             2          Basic  \n",
      "5                   0.6            10         Active  \n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüéØ DEDUPLICATION STRATEGY:\")\n",
    "print(\"   ‚Ä¢ Keep most recent registration (latest date)\")\n",
    "print(\"   ‚Ä¢ Break ties by preferring higher profile completeness\")\n",
    "print(\"   ‚Ä¢ Ensure one record per customer\")\n",
    "\n",
    "# Apply intelligent deduplication\n",
    "cleaned_data = dut.deduplicate_by_rank(\n",
    "    customer_data,\n",
    "    id_cols='customer_id',\n",
    "    ranking_col='registration_date',\n",
    "    ascending=False,  # Keep most recent\n",
    "    tiebreaker_col='email',  # Prefer non-null emails in case of ties\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä DEDUPLICATION RESULTS:\")\n",
    "print(f\"   ‚Ä¢ Original records: {len(customer_data)}\")\n",
    "print(f\"   ‚Ä¢ After deduplication: {len(cleaned_data)}\")\n",
    "print(f\"   ‚Ä¢ Records removed: {len(customer_data) - len(cleaned_data)}\")\n",
    "\n",
    "print(f\"\\n‚úÖ CLEANED CUSTOMER DATA:\")\n",
    "print(cleaned_data[['customer_id', 'email', 'registration_date', 'profile_completeness', 'total_orders', 'account_status']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 4: VERIFY - Confirm Data Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Verifying cleaned data quality...\n",
      "‚úÖ There are no missing values in column 'customer_id'.\n",
      "‚ÑπÔ∏è Total row count after filtering out missings: 6\n",
      "‚ÑπÔ∏è Unique row count after filtering out missings: 6\n",
      "üîë The column(s) 'customer_id' form a primary key.\n",
      "\n",
      "üéâ SUCCESS! Data quality issues resolved:\n",
      "   ‚úÖ customer_id is now a proper primary key\n",
      "   ‚úÖ No duplicate customers remain\n",
      "   ‚úÖ Most recent and complete records preserved\n",
      "   ‚úÖ Data ready for ML pipeline processing\n",
      "\n",
      "======================================================================\n",
      "WORKFLOW SUMMARY\n",
      "======================================================================\n",
      "\n",
      "üìã COMPLETE DUPLICATE MANAGEMENT WORKFLOW:\n",
      "   1. üîç DISCOVER  ‚Üí Used is_primary_key() to detect duplication issues\n",
      "   2. üìä INVESTIGATE ‚Üí Used find_duplicates() to analyze patterns\n",
      "   3. üéØ RESOLVE   ‚Üí Used deduplicate_by_rank() to intelligently clean data\n",
      "   4. ‚úÖ VERIFY    ‚Üí Confirmed data quality with is_primary_key()\n",
      "\n",
      "üéØ BUSINESS IMPACT:\n",
      "   ‚Ä¢ Reduced dataset from 9 to 6 records\n",
      "   ‚Ä¢ Preserved most recent and complete customer information\n",
      "   ‚Ä¢ Eliminated duplicate customers for accurate analytics\n",
      "   ‚Ä¢ Ensured data quality for downstream ML processes\n",
      "\n",
      "üí° KEY BENEFITS:\n",
      "   ‚Ä¢ Systematic approach to data quality management\n",
      "   ‚Ä¢ Business logic embedded in deduplication strategy\n",
      "   ‚Ä¢ Transparent process with detailed reporting\n",
      "   ‚Ä¢ Production-ready cleaned dataset\n"
     ]
    }
   ],
   "source": [
    "# Verify the cleaned data\n",
    "print(\"\\nüîç Verifying cleaned data quality...\")\n",
    "is_clean = dut.is_primary_key(cleaned_data, 'customer_id', verbose=True)\n",
    "\n",
    "if is_clean:\n",
    "    print(\"\\nüéâ SUCCESS! Data quality issues resolved:\")\n",
    "    print(\"   ‚úÖ customer_id is now a proper primary key\")\n",
    "    print(\"   ‚úÖ No duplicate customers remain\")\n",
    "    print(\"   ‚úÖ Most recent and complete records preserved\")\n",
    "    print(\"   ‚úÖ Data ready for ML pipeline processing\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"WORKFLOW SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nüìã COMPLETE DUPLICATE MANAGEMENT WORKFLOW:\")\n",
    "print(\"   1. üîç DISCOVER  ‚Üí Used is_primary_key() to detect duplication issues\")\n",
    "print(\"   2. üìä INVESTIGATE ‚Üí Used find_duplicates() to analyze patterns\") \n",
    "print(\"   3. üéØ RESOLVE   ‚Üí Used deduplicate_by_rank() to intelligently clean data\")\n",
    "print(\"   4. ‚úÖ VERIFY    ‚Üí Confirmed data quality with is_primary_key()\")\n",
    "\n",
    "print(f\"\\nüéØ BUSINESS IMPACT:\")\n",
    "print(f\"   ‚Ä¢ Reduced dataset from {len(customer_data)} to {len(cleaned_data)} records\")\n",
    "print(f\"   ‚Ä¢ Preserved most recent and complete customer information\")\n",
    "print(f\"   ‚Ä¢ Eliminated duplicate customers for accurate analytics\")\n",
    "print(f\"   ‚Ä¢ Ensured data quality for downstream ML processes\")\n",
    "\n",
    "print(f\"\\nüí° KEY BENEFITS:\")\n",
    "print(f\"   ‚Ä¢ Systematic approach to data quality management\")\n",
    "print(f\"   ‚Ä¢ Business logic embedded in deduplication strategy\")\n",
    "print(f\"   ‚Ä¢ Transparent process with detailed reporting\")\n",
    "print(f\"   ‚Ä¢ Production-ready cleaned dataset\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
