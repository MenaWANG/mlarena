{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `mlarena.utils.io_utils` Demo\n",
    "\n",
    "This notebook serves as a demonstration of the various utilities available in the `mlarena.utils.io_utils` module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mlarena.utils.io_utils as iou\n",
    "from mlarena import PreProcessor, MLPipeline\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier\n",
    ") \n",
    "from sklearn.datasets import (\n",
    "    fetch_openml\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. `save_object` and `load_object`\n",
    "\n",
    "The `save_object` and `load_object` functions provide a convenient way to save and load Python objects to/from disk. These functions are particularly useful for:\n",
    "\n",
    "- Saving trained machine learning models for later use\n",
    "- Storing intermediate results or processed data\n",
    "- Archiving complex Python objects (dictionaries, DataFrames, etc.)\n",
    "- Sharing data between different Python sessions or scripts\n",
    "\n",
    "The functions support two backends:\n",
    "- `pickle`: Python's built-in serialization format\n",
    "- `joblib`: Optimized for large numerical data and scientific computing objects\n",
    "\n",
    "Key features include:\n",
    "- Streamlined backend handling:\n",
    "    - With `save_object`: Automatic file extension handling based on backend\n",
    "    - With `load_object`: Backend automatically detected \n",
    "- Optional date stamping of saved files\n",
    "- Compression support for joblib backend\n",
    "- Simple and consistent interface for both saving and loading\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Save & load a dictionary with pickle backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object saved to demo_outputs\\data_dict_2025-05-29.pkl\n",
      "Object loaded from demo_outputs\\data_dict_2025-05-29.pkl\n",
      "Data integrity check: True\n"
     ]
    }
   ],
   "source": [
    "# Create a sample dictionary with mixed data types\n",
    "data_dict = {\n",
    "    \"name\": \"example\",\n",
    "    \"values\": [1, 2, 3, 4, 5],\n",
    "    \"active\": True,\n",
    "    \"metadata\": {\n",
    "        \"version\": \"1.0\",\n",
    "        \"timestamp\": \"2024-03-20\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save the dictionary using pickle backend (default)\n",
    "filepath1 = iou.save_object(\n",
    "    data_dict,\n",
    "    directory=\"demo_outputs\",\n",
    "    basename=\"data_dict\",\n",
    "    use_date=True  # Include date in filename\n",
    ")\n",
    "\n",
    "# Load the saved dictionary\n",
    "data_dict_retrieved = iou.load_object(filepath1)\n",
    "\n",
    "# Verify data integrity\n",
    "print(\"Data integrity check:\", data_dict == data_dict_retrieved)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Save & load a pandas df with joblib backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object saved to demo_outputs\\sample_df_2025-05-29.joblib\n",
      "Object loaded from demo_outputs\\sample_df_2025-05-29.joblib\n",
      "\n",
      "Data integrity check: True\n"
     ]
    }
   ],
   "source": [
    "# Create sample data\n",
    "dates = pd.date_range(start='2024-01-01', periods=5)\n",
    "df = pd.DataFrame({\n",
    "    'date': dates,\n",
    "    'category': ['A', 'B', 'C', 'A', 'B'],\n",
    "    'value': np.random.randn(5),\n",
    "    'count': np.random.randint(1, 100, 5),\n",
    "    'is_active': [True, False, True, True, False]\n",
    "})\n",
    "\n",
    "# Save the DataFrame using joblib backend with compression\n",
    "filepath2 = iou.save_object(\n",
    "    df,\n",
    "    directory=\"demo_outputs\",\n",
    "    basename=\"sample_df\",\n",
    "    backend=\"joblib\",  # Use joblib backend\n",
    "    compress=True,     # Enable compression\n",
    "    use_date=True     # Include date in filename\n",
    ")\n",
    "\n",
    "# Load the saved DataFrame, the backend is detected automatically\n",
    "df_retrieved = iou.load_object(filepath2)\n",
    "\n",
    "# Verify data integrity\n",
    "print(\"\\nData integrity check:\", df.equals(df_retrieved))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Saving and loading a MLPipeline instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object saved to demo_outputs\\titanic_pipeline_2025-05-29.joblib\n",
      "Object loaded from demo_outputs\\titanic_pipeline_2025-05-29.joblib\n",
      "\n",
      "Pipeline integrity check: True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "titanic = fetch_openml('titanic', version=1, as_frame=True)\n",
    "X = titanic.data\n",
    "y = titanic.target.astype(int)  \n",
    "X = X.drop(['boat', 'body', 'home.dest', 'ticket', 'cabin', 'name'], axis=1)\n",
    "X = PreProcessor.mlflow_input_prep(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "ml_pipeline = MLPipeline(\n",
    "    model = RandomForestClassifier(),\n",
    "    preprocessor = PreProcessor()\n",
    "    )\n",
    "# fit pipeline\n",
    "ml_pipeline.fit(X_train,y_train)\n",
    "\n",
    "# Save the ML pipeline using joblib backend with compression\n",
    "pipeline_filepath = iou.save_object(\n",
    "    ml_pipeline,\n",
    "    directory=\"demo_outputs\",\n",
    "    basename=\"titanic_pipeline\",\n",
    "    backend=\"joblib\",\n",
    "    compress=True,\n",
    "    use_date=True\n",
    ")\n",
    "\n",
    "# Load the saved pipeline\n",
    "pipeline_retrieved = iou.load_object(pipeline_filepath)\n",
    "\n",
    "# Verify pipeline integrity by comparing predictions\n",
    "original_preds = ml_pipeline.predict(model_input = X_test, context = None)\n",
    "retrieved_preds = pipeline_retrieved.predict(model_input = X_test, context = None)\n",
    "\n",
    "print(\"\\nPipeline integrity check:\", np.array_equal(original_preds, retrieved_preds))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".test_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
