# Removing Disruptors So Your Model Learns the True Signal
# ‚Äî An Algorithm-Agnostic Approach Inspired by Cook's Distance

**Imagine this:** You're modeling loan default risk using income and credit history. A few borrowers with relatively low incomes seem to repay large loans just fine ‚Äî which could mislead the model. In reality, they had submitted their income in US dollars rather than your local currency, but this was missed during data entry, making them appear less creditworthy than they actually were. 

Or you're building a model to predict patient recovery times. Most patients follow expected recovery trajectories ‚Äî but a few experienced rare complications that weren‚Äôt recorded. These cases sit far from the rest in terms of the relationship between symptoms, treatments, and outcomes. They're not necessarily ‚Äúwrong,‚Äù but they are disruptive ‚Äî causing the model to generalize poorly to the majority of future patients.

In both cases, the issue isn‚Äôt just noise or classic anomalies. The problem is more subtle:

> **Some observations disproportionately disrupt the model‚Äôs ability to learn the dominant signal.**

These points may:

* Have **disproportionate influence** on the learned parameters,
* Come from **unusual or unmodeled contexts** (e.g., rare complications or data entry issues),
* And most importantly, **reduce the model's ability to generalize**.

**üéØ What We Seek to Achieve Here**

In this article, I‚Äôll walk through an approach to effectively **identify and remove these disruptive points**, so the model can better capture the stable, generalizable patterns in the data. This method is **algorithm-agnostic**, so we can leverage it for any algorithms or analytical framework we have chosen for our use case. 

By the end of this article, I hope you‚Äôll see how this simple yet powerful technique can make your models more robust and trustworthy ‚Äî by helping them focus on the signal, not the noise.


## Inspiration: Cook‚Äôs Distance, Reimagined

Cook‚Äôs Distance is a classic diagnostic tool from linear regression. It tells us how much a single data point influences the model by:

- Training the model on the full dataset  
- Retraining it with one data point left out  
- Measuring how much the predictions change

A large Cook‚Äôs Distance means that point has high influence‚Äîpossibly distorting the model. But this technique has limitations:

- It only works with ordinary least squares (OLS) regression  
- Assumes normally distributed errors  
- Is sensitive to the scale of the target variable

We can generalize Cook‚Äôs idea using modern, model-agnostic techniques:

- Use any supervised estimator (e.g., XGBoost, LightGBM, neural nets)  
- For each point, compute the squared difference between the full-model prediction and the prediction from a model trained with that point left out  
- Apply strategies to reduce computational cost for large datasets when necessary
- Visualize the results to identify influential or disruptive observations
- Optionally, standardize the target to reduce sensitivity to magnitude  

The result is a versatile approach that measures disruptive influence on learned relationships. It‚Äôs Cook‚Äôs intuition, expanded for modern modeling.

## Why This Approach?

This Cook's-like influence approach is uniquely suited for identifying data points that distort a model's learned patterns, a gap often left by other outlier detection techniques.

* **Beyond Univariate Detection:** Univariate methods (like Z-scores or IQR rules) identify extreme values within individual features or the target variable alone. However, points that significantly influence a complex model's prediction may appear perfectly ordinary when each of their features is examined in isolation. They are "outliers" not by their individual values, but by their *relationship* to the overall data and the model's structure.

* **Beyond Feature-Focused Anomaly Detection:** Techniques such as Isolation Forest or Local Outlier Factor (LOF) excel at detecting anomalies purely based on the distribution and density of input features ($X$). While valuable for identifying unusual data entries, they inherently **do not consider the role of the target variable ($Y$) or how a model uses features to predict it.** Consequently, a data point flagged as an outlier in the feature space might not necessarily have a disproportionate impact on your model's predictive performance or overall learned pattern. Conversely, a point not flagged by these methods might still be highly influential on the model's predictions.

* **Beyond Standard Residual-Based Methods:** Residuals, the difference between actual and predicted values, highlight where the model performs poorly. While this indicates a deviation, it doesn‚Äôt distinguish between whether the point is simply noisy (e.g., unpredictable but harmless) or truly *disrupting*, that is, "pulling" the model's entire predictive surface away from the general pattern established by the majority of the data. We could have points with high residuals but little influence; or those with moderate residuals but disproportionately warp the model‚Äôs fit.

This is where a Cook‚Äôs-style influence metric truly shines. It goes beyond the size of the prediction error to ask:

> **How structurally destabilizing a single data point is to the entire model's learned relationships.**

 Such an approach enables us to surgically identify and remove data points that disproportionately pull the model's predictions away from the "general pattern" reflected in the rest of the data. This capability is particularly vital in context where robustness and generalization matter more than raw accuracy, such as in:

* **Pricing models:** Where a few anomalous transactions could skew pricing rules for many products.
* **Diagnostic tools:** Where a few unusual patient records may bias predictions for the broader population.
* **Predictive maintenance:** Where a few one-off failure events could miscalibrate a model, leading to costly premature maintenance or unexpected breakdowns.

**In essence, while other methods help us find "weird" data, the Cook's-like approach helps us find data points that make our *model itself* "weird" in its overall behavior.**

### Digging Deeper: How This Compares to Other Influence Metrics

In summary, the underlying principle of Cook's Distance is the most aligned with our goal of identifying and removing data points that distort a model's learning. If you're mainly here to implement the approach, feel free to jump straight to the next section on  implementation. But if you are curious how it compares to other influence metrics, let's dig deeper together. 

This section offers a conceptual and practical comparison of several algorithms for assessing the influence of individual data points on model learning.

**Summary of Algorithms/Metrics for Assessing Data Point Influence**


| Metric/Algorithm | What it Measures | How it Works | Key Interpretation | Primary Applicability | Notes |
|---|---|---|---|---|---|
| **Cook's Distance (D)** | Overall influence of an observation on the model's fitted values (predictions). | Quantifies how much all fitted values would change if the observation were removed. Combines residual and leverage. | Larger D indicates more influence. Rules of thumb: $D > 4/n$ or $D > 1$. | Primarily Linear Regression | A single value per observation, reflecting its global impact. |
| **DFBETAS** | Influence of an observation on each *individual* regression coefficient. | Measures the standardized change in each coefficient when a specific observation is excluded. | Large absolute DFBETAS values indicate significant influence on that specific coefficient. Rule of thumb: $|DFBETAS| > 2/\sqrt{n}$. | Primarily Linear Regression | Provides insight into which parameters are most affected by a given point. |
| **DFFITS** | Influence of an observation on its *own* predicted value. | Measures how much the predicted value for a specific observation changes when that observation is removed and the model is refit. | Larger DFFITS values indicate higher influence on that point's own prediction. | Primarily Linear Regression | Similar to Cook's D but focuses on the impact on the specific point's prediction. |
| **Leverage ($h_{ii}$)** | How "unusual" an observation's independent variable values are, indicating its potential to influence the model. | Derived from the hat matrix in linear regression. High values mean the point is far from the mean of predictor values. | High leverage points have the potential to be influential. Rule of thumb: $h_{ii} > 2p/n$ or $h_{ii} > 3p/n$. | Primarily Linear Regression | A high leverage point is not necessarily influential by itself; it needs a large residual to be truly influential. |
| **Leave-One-Out Cross-Validation (LOOCV)** | Direct assessment of an individual data point's impact on overall model performance/prediction accuracy. | Model is trained on all data points except one, then tested on the held-out point. Repeated for all points. | If model performance drastically changes when a point is excluded, that point is influential. | General Machine Learning Models (computationally intensive) | Provides a direct measure of how vital a single point is to the model's predictive power. |
| **Influence Functions** | Theoretical measure of how much an infinitesimal perturbation of a training point affects model parameters or predictions. | Mathematically approximates the change in model output due to a small change in a single data point. | Larger influence values indicate greater sensitivity to that data point. | More Complex ML Models (e.g., Neural Networks, SVMs) | More advanced and computationally intensive; offers insights into influence in non-linear models. |

**Why This ‚ÄúCook‚Äôs-like‚Äù Influence Method Stands Out**

For our use case, where the goal is to **identify data points that disproportionately pull the model's predictions away from the "general pattern" reflected in the rest of the data**, the Cook's-like algorithm is  the winner. Here is why

* **What it Measures for our Use Case:** This algorithm measures the **aggregate change in the model's predictions across *all* data points** when a single observation is removed from the training set and the model is refit. It directly quantifies how much a specific data point affects the *overall learned pattern* of the model for the entire dataset.
* **How it Works:**
    1.  Train your chosen machine learning model (e.g., LightGBM, Random Forest, Neural Network, Linear Regression) on the full dataset, $D$. Record all the predictions, $\hat{Y}_{full}$.
    2.  For each data point $i$ in your dataset:
        a.  Temporarily remove point $i$ to create a new dataset, $D_{-i}$.
        b.  Train a new instance of your model on $D_{-i}$.
        c.  Use this new model to generate predictions for *all* data points in the original full dataset $D$ (or at least the remaining $D_{-i}$), calling these $\hat{Y}_{(-i)}$.
        d.  Calculate a metric of divergence between $\hat{Y}_{full}$ and $\hat{Y}_{(-i)}$. The most common and direct approach, inspired by Cook's Distance, is the sum of squared differences of predictions:
            $$\text{Influence}_i = \sum_{j=1}^{n} (\hat{y}_{j, \text{full}} - \hat{y}_{j, (-i)})^2$$
            where $\hat{y}_{j, \text{full}}$ is the prediction for point $j$ from the full model, and $\hat{y}_{j, (-i)}$ is the prediction for point $j$ from the model trained without point $i$.
* **Applicability for "Identifying General Pattern Deviators":** **Excellent and Direct.** This method directly answers how much the model's understanding of the "general pattern" changes across the *entire dataset* if a specific point is included versus excluded. A high $\text{Influence}_i$ value indicates that data point $i$ is causing the model's predictions to significantly deviate from what they would otherwise be for the collective data, thus distorting the "general pattern."
* **Notes:** This approach is computationally expensive as it requires retraining the model $N$ times. For large datasets, a pragmatic strategy is to first identify potential outliers using residual analysis or other relevant algorithms (see more on this in the implementation section later), and then apply this "Cook's-like" measure only to the subset of suspected influential points. This makes the approach feasible while retaining its powerful diagnostic capability.

**Summary of Algorithms/Metrics from the Perspective of Our Goal**

Because it quantifies the global predictive shift caused by each data point, this Cook's-like approach provides a uniquely precise lens for identifying true pattern deviators. The table below distills how well each method maps to our core objective of identifying such data points.


| Metric/Algorithm | What it Measures | Applicability for "Identifying General Pattern Deviators" | Why / Why Not |
|---|---|---|---|
| **Cook's Distance (D)** | Overall influence of an observation on the model's *entire set of fitted values* (predictions). | **Highly Relevant (Conceptual Winner)** | This is the conceptual ideal. It directly measures how much *all* predictions would shift if a point were removed, aligning perfectly with "deviate from the general pattern from all the rest data points." **It is the inspiration for the "Cook's-like" algorithm for general models.** |
| **DFBETAS** | Influence of an observation on each *individual regression coefficient*. | **Limited / Indirect** | Less direct. It tells you about parameter stability, not directly about the overall shift in predictions for *all* data points. Also, it's specific to linear models. |
| **DFFITS** | Influence of an observation on its *own* predicted value. | **Limited / Indirect** | While useful for finding points that highly influence their own prediction, it doesn't quantify the impact on the "general pattern" for *all other data points*. It's also specific to linear models. |
| **Leverage ($h_{ii}$)** | How "unusual" an observation's independent variable values are, indicating its *potential* to influence the model. | **Indirect / Complementary** | It identifies points that *could* be influential due to their position in the feature space, but not their actual impact on predictions. High leverage combined with a large residual often indicates influence. Not a direct measure for our goal on its own. |
| **Leave-One-Out Cross-Validation (LOOCV)** | Direct assessment of an individual data point's impact on *overall model performance* (e.g., MSE, R2 on the full test set). | **Highly Relevant (Practical Approach)** | If you use LOOCV to compare the *overall model performance* (e.g., how the model trained on $D_{-i}$ performs on the full validation set) rather than just the single prediction, it's very relevant. If removing a point significantly improves overall performance, it suggests it was distorting the "general pattern." |
| **Influence Functions** | Theoretical measure of how much an infinitesimal perturbation of a training point affects model parameters or predictions. | **Highly Relevant (Theoretical/Advanced)** | In principle, this can capture global impact. However, it's typically more complex to implement and computationally demanding, often requiring approximations for non-linear models. Good for research, less for general practical application. |

## Implementation and Example

Now let's discuss the implementation of the **Algorithm-Agnostic "Cook's-like" Measure** to pinpoint data points that significantly distort a model's learned patterns. This method directly extends the foundational concept of Cook's Distance from linear regression to any machine learning algorithm, allowing us to quantify a single data point's global impact on the model's overall predictions.

**The Core Idea:**
At its heart, this approach asks: "If we remove a single data point from the training set and re-train the model, how much do the predictions for *all other data points* change compared to when that point was included?" A large change signifies high influence.

**The Process:**

1.  **Train the Full Model:**
    First, we train our chosen machine learning model (e.g., LightGBM, Random Forest, Neural Network, Linear Regression) on the entire dataset, $D$. This yields a set of baseline predictions for all data points: $\hat{Y}_{\text{full}} = (\hat{y}_{1, \text{full}}, \hat{y}_{2, \text{full}}, \ldots, \hat{y}_{n, \text{full}})$. These predictions represent the model's understanding of the "general pattern" with all data considered.

2.  **Iterate and Train Leave-One-Out (LOO) Models:**
    For each data point $i$ from $1$ to $N$ (where $N$ is the total number of data points), we perform the following steps:
    a.  **Create a Subset:** We create a new dataset, $D_{-i}$, by removing only data point $i$ from the original training set.
    b.  **Retrain the Model:** A new instance of our chosen model is trained on this reduced dataset, $D_{-i}$.
    c.  **Generate New Predictions:** Using this newly trained model (which has not "seen" point $i$), we generate predictions for *all* data points in the original full dataset, $D$. Let's call these $\hat{Y}_{(-i)} = (\hat{y}_{1, (-i)}, \hat{y}_{2, (-i)}, \ldots, \hat{y}_{n, (-i)})$.
    d.  **Calculate Influence Score:** We then quantify the "influence" of data point $i$ by calculating the sum of squared differences between the predictions from the full model and the predictions from the model trained without point $i$:
        $$\text{Influence}_i = \sum_{j=1}^{n} (\hat{y}_{j, \text{full}} - \hat{y}_{j, (-i)})^2$$
        (While this is typically a sum of squared differences for regression tasks, similar metrics like changes in classification probabilities or overall model performance metrics can be adapted for other problem types.)

3.  **Identify Influential Points:**
    Data points with a large $\text{Influence}_i$ score are considered highly influential. These are the points whose presence significantly alters the model's overall learned pattern.

**Why This Approach is Powerful:**

* **Truly Algorithm-Agnostic:** This method works universally with *any* machine learning model that can be trained and used for prediction. It doesn't rely on model-specific internal calculations (like the standard error of coefficients in linear regression).
* **Directly Addresses "General Pattern" Distortion:** By comparing the full model's predictions to the LOO model's predictions across *all data points*, it directly measures how much the model's overall understanding of the data relationships (its "general pattern") shifts when a single point is either included or excluded. This aligns perfectly with our objective.

**Addressing the Computational Challenge:**

The primary limitation of this powerful approach is its **computational cost**, as it necessitates $N$ full model retrainings. For large datasets, this can be prohibitively expensive.

To mitigate this, the below implementation introduces an optional `max_loo_points` parameter. Instead of processing every single point, user can choose to employ a **residual-based pre-filtering strategy**:

* **`max_loo_points` (integer, optional):** If specified, the full LOO calculation is performed only for this many data points. These points are selected because they exhibit the highest absolute residuals (the largest errors) when predicted by the initial full model. This effectively focuses the computationally intensive step on the most likely influential candidates.
* **`residual_outlier_method` (string, 'percentile' or 'isolation_forest'):** This parameter determines how the high-residual points are identified.
    * `'percentile'`: Selects points whose absolute residuals exceed a specified `residual_threshold` percentile (e.g., 99th percentile for the top 1%).
    * `'isolation_forest'`: Utilizes the Isolation Forest algorithm to robustly detect outliers within the distribution of absolute residuals, selecting the most anomalous ones.
* **`residual_threshold` (float):** Used in conjunction with `residual_outlier_method='percentile'` to define the percentile cutoff for identifying high residuals.
* **`random_state` (integer, optional):** Ensures reproducibility when models or outlier detection methods (like Isolation Forest) involve randomness.

This pre-filtering mechanism makes the Cook's-like influence calculation practical for larger datasets, allowing us to find the most distorting points without the burden of $N$ retrainings for all data.

```
import numpy as np
import pandas as pd
from sklearn.model_selection import LeaveOneOut
from sklearn.linear_model import LinearRegression
from lightgbm import LGBMRegressor
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import IsolationForest 
import matplotlib.pyplot as plt

def calculate_cooks_like_influence(model_class, X, y, 
                                   visualize=True, 
                                   save_path=None, 
                                   max_loo_points=None, 
                                   residual_outlier_method='percentile', 
                                   residual_threshold=99, 
                                   random_state=None, 
                                   **model_params):
    """
    Calculates a Cook's-like influence score for each data point
    for any scikit-learn compatible model.

    Args:
        model_class: The class of the ML model to use (e.g., LinearRegression, LGBMRegressor).
        X (pd.DataFrame or np.array): The feature matrix.
        y (pd.Series or np.array): The target vector.
        visualize (bool): If True, plots the influence scores. Defaults to True.
        save_path (str, optional): If provided, saves the plot to this file path.
                                  Requires visualize=True. Defaults to None.
        max_loo_points (int, optional): If specified, only perform LOO calculations for
                                        this many points. These points are selected based on
                                        having the highest residuals. If None, performs for all.
                                        Defaults to None.
        residual_outlier_method (str): Method to select high-residual points if max_loo_points is set.
                                       'percentile' (uses residual_threshold) or 'isolation_forest'.
                                       Defaults to 'percentile'.
        residual_threshold (float): Percentile to use for filtering high-residual points
                                    if residual_outlier_method is 'percentile'. E.g., 99 for top 1%.
                                    Defaults to 99.
        random_state (int, optional): Random state for IsolationForest if used, and for the model_class if it accepts it.
        **model_params: Keyword arguments to pass to the model constructor.

    Returns:
        np.array: An array of influence scores, one for each data point.
    """
    n_samples = X.shape[0]
    influence_scores = np.zeros(n_samples)

    # 1. Train the full model
    print(f"Training full model using {model_class.__name__}...")
    full_model = model_class(random_state=random_state if 'random_state' in model_class.__init__.__code__.co_varnames else None, **model_params)
    full_model.fit(X, y)
    y_pred_full = full_model.predict(X)

    # Calculate initial residuals
    residuals = np.abs(y - y_pred_full) 

    # Determine which points to perform LOO on
    loo_indices_to_process = []
    if max_loo_points is None or max_loo_points >= n_samples:
        loo_indices_to_process = np.arange(n_samples) # Process all points
        print("Performing Cook's-like influence calculation for ALL points.")
    else:
        print(f"Identifying up to {max_loo_points} points with highest residuals for LOO calculation...")
        if residual_outlier_method == 'percentile':
            threshold_value = np.percentile(residuals, residual_threshold)
            high_residual_indices = np.where(residuals >= threshold_value)[0]
            if len(high_residual_indices) > max_loo_points:
                top_indices = np.argsort(residuals)[::-1][:max_loo_points]
                loo_indices_to_process = top_indices
            else:
                loo_indices_to_process = high_residual_indices
            print(f"  Selected {len(loo_indices_to_process)} points based on {residual_threshold}th percentile of residuals.")
        elif residual_outlier_method == 'isolation_forest':
            iso_forest = IsolationForest(random_state=random_state, contamination='auto')
            iso_forest.fit(residuals.reshape(-1, 1))
            outlier_scores = iso_forest.decision_function(residuals.reshape(-1, 1)) 
            top_indices_by_iso_forest = np.argsort(outlier_scores)[:max_loo_points] 
            loo_indices_to_process = top_indices_by_iso_forest
            print(f"  Selected {len(loo_indices_to_process)} points based on Isolation Forest on residuals.")
        else:
            raise ValueError("Invalid residual_outlier_method. Choose 'percentile' or 'isolation_forest'.")

    loo_indices_to_process = np.array(loo_indices_to_process) 

    print(f"Calculating Cook's-like influence for {len(loo_indices_to_process)} selected points using {model_class.__name__}...")
    # Using boolean mask for efficient pandas subsetting
    loo_mask = np.ones(n_samples, dtype=bool) 
    
    for i_idx, original_data_index in enumerate(loo_indices_to_process):
        loo_mask[original_data_index] = False # Set current point to False

        X_train_loo = X.iloc[loo_mask]
        y_train_loo = y.iloc[loo_mask]

        loo_model = model_class(random_state=random_state if 'random_state' in model_class.__init__.__code__.co_varnames else None, **model_params)
        loo_model.fit(X_train_loo, y_train_loo)

        y_pred_loo_on_full_data = loo_model.predict(X)
        influence_scores[original_data_index] = mean_squared_error(y_pred_full, y_pred_loo_on_full_data) * n_samples 
        
        # Reset mask for next iteration (crucial!)
        loo_mask[original_data_index] = True 

        if (i_idx + 1) % (len(loo_indices_to_process) // 10 + 1) == 0 or i_idx == len(loo_indices_to_process) - 1:
            print(f"  Processed {i_idx + 1}/{len(loo_indices_to_process)} selected samples.")

    if visualize:
        fig, ax = plt.subplots(figsize=(12, 6))
        ax.plot(influence_scores)
        ax.set_title(f"Cook's-like Influence Scores for {model_class.__name__}")
        ax.set_xlabel("Data Point Index")
        ax.set_ylabel("Influence Score (Sum of Squared Differences)")

        calculated_scores = influence_scores[loo_indices_to_process]
        if len(calculated_scores) > 0: 
            percentile_95 = np.percentile(calculated_scores, 95)
            ax.axhline(y=percentile_95, color='r', linestyle='--', label=f'95th Percentile ({percentile_95:.2f}) of calculated scores')
            ax.legend()

        ax.grid(True, linestyle=':', alpha=0.7)
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path)
            print(f"Plot saved to {save_path}")
        
        plt.show()
        plt.close(fig) # Use fig.close() to explicitly close the figure

    return influence_scores

# --- New Wrapper Function ---
def get_normal_data(model_class, X, y, 
                    influence_threshold_percentile=99, # New parameter for filtering "normal" data
                    visualize_influence=True, 
                    save_path_influence_plot=None, 
                    max_loo_points=None, 
                    residual_outlier_method='percentile', 
                    residual_threshold=99, 
                    random_state=None, 
                    **model_params):
    """
    Identifies influential data points using the Cook's-like approach and returns
    the 'normal' (non-influential) subset of X and y for model training.

    Args:
        model_class: The class of the ML model to use (e.g., LinearRegression, LGBMRegressor).
        X (pd.DataFrame or np.array): The feature matrix.
        y (pd.Series or np.array): The target vector.
        influence_threshold_percentile (float): The percentile threshold for influence scores.
                                                Points with influence scores above this
                                                percentile will be considered influential
                                                and excluded from the 'normal' dataset.
                                                Defaults to 99 (top 1% influential).
        visualize_influence (bool): If True, plots the influence scores. Defaults to True.
        save_path_influence_plot (str, optional): If provided, saves the influence plot.
        max_loo_points (int, optional): Passed to calculate_cooks_like_influence.
        residual_outlier_method (str): Passed to calculate_cooks_like_influence.
        residual_threshold (float): Passed to calculate_cooks_like_influence.
        random_state (int, optional): Passed to calculate_cooks_like_influence.
        **model_params: Keyword arguments to pass to the model constructor.

    Returns:
        tuple: A tuple (X_normal, y_normal) containing the features and target
               of the non-influential data points.
        np.array: The full array of calculated influence scores.
    """
    print("\n--- Starting Influence Analysis to Identify Normal Data ---")

    # Calculate influence scores
    influence_scores = calculate_cooks_like_influence(
        model_class=model_class,
        X=X,
        y=y,
        visualize=visualize_influence,
        save_path=save_path_influence_plot,
        max_loo_points=max_loo_points,
        residual_outlier_method=residual_outlier_method,
        residual_threshold=residual_threshold,
        random_state=random_state,
        **model_params
    )

    # Determine the threshold for influential points
    # Only consider points where scores were calculated for thresholding
    calculated_scores_for_threshold = influence_scores[influence_scores > 0] # Exclude zero scores from uncalculated points
    if len(calculated_scores_for_threshold) == 0:
        print("No influence scores were calculated (perhaps max_loo_points was 0 or data too small). Returning original data.")
        return X, y, influence_scores

    influence_threshold_value = np.percentile(calculated_scores_for_threshold, influence_threshold_percentile)
    
    # Identify non-influential points (scores below threshold)
    normal_indices = np.where(influence_scores <= influence_threshold_value)[0]

    X_normal = X.iloc[normal_indices]
    y_normal = y.iloc[normal_indices]

    print(f"\n--- Influence Analysis Complete ---")
    print(f"Total data points: {len(X)}")
    print(f"Influence threshold (at {influence_threshold_percentile}th percentile): {influence_threshold_value:.4f}")
    print(f"Number of influential points removed: {len(X) - len(X_normal)}")
    print(f"Remaining 'normal' data points: {len(X_normal)}")

    return X_normal, y_normal, influence_scores

# --- Example Usage with Wrapper ---
if __name__ == "__main__":
    np.random.seed(42)
    X_data = pd.DataFrame(np.random.rand(500, 5) * 10, columns=[f'feature{i}' for i in range(5)])
    y_data = (2 * X_data['feature1'] + 3 * X_data['feature2'] - 1 * X_data['feature3'] + 
              0.5 * X_data['feature4'] + np.random.randn(500) * 2)

    # Introduce a few influential outliers
    X_data.loc[99] = [1, 1, 1, 1, 1]
    y_data.loc[99] = 1000

    X_data.loc[250] = [50, 50, 50, 50, 50]
    y_data.loc[250] = -500

    X_data.loc[400] = [10, 10, 10, 10, 10]
    y_data.loc[400] = 500

    print("\n--- Running get_normal_data for Linear Regression ---")
    X_normal_lr, y_normal_lr, lr_all_influence = get_normal_data(
        model_class=LinearRegression,
        X=X_data,
        y=y_data,
        influence_threshold_percentile=98, # Remove top 2% of influential points
        max_loo_points=50, # Only consider top 50 residual points for influence calc
        visualize_influence=True,
        save_path_influence_plot="lr_normal_data_influence.png"
    )

    print("\nShape of original data:", X_data.shape, y_data.shape)
    print("Shape of normal data (Linear Regression):", X_normal_lr.shape, y_normal_lr.shape)

    # train the final model on X_normal_lr, y_normal_lr
    final_lr_model = LinearRegression()
    final_lr_model.fit(X_normal_lr, y_normal_lr)
    print("\nFinal LR model trained on normal data.")

    print("\n--- Running get_normal_data for LightGBM Regressor ---")
    X_normal_lgbm, y_normal_lgbm, lgbm_all_influence = get_normal_data(
        model_class=LGBMRegressor,
        X=X_data,
        y=y_data,
        influence_threshold_percentile=99, # Remove top 1%
        n_estimators=100,
        random_state=42,
        max_loo_points=70, # Consider top 70 residual points
        residual_outlier_method='isolation_forest', # Use Isolation Forest for residual filtering
        visualize_influence=True,
        save_path_influence_plot="lgbm_normal_data_influence.png"
    )

    print("\nShape of original data:", X_data.shape, y_data.shape)
    print("Shape of normal data (LightGBM):", X_normal_lgbm.shape, y_normal_lgbm.shape)
    
    final_lgbm_model = LGBMRegressor(n_estimators=100, random_state=42)
    final_lgbm_model.fit(X_normal_lgbm, y_normal_lgbm)
    print("\nFinal LGBM model trained on normal data.")

    ```