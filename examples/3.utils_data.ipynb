{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `mlarena.utils.data_utils` Demo\n",
    "\n",
    "This notebook serves as a demonstration of the various data cleaning and manipulation utilities available in the `mlarena.utils.data_utils` module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlarena.utils.data_utils as dut\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Transform Data Columns\n",
    "\n",
    "It is common for a dataframe to have date columns stored as strings. This handy function `transform_date_cols` helps you transform them. \n",
    "\n",
    "- Flexible input handling: Works with either a single column or multiple columns\n",
    "- Format customization: Supports any date format using standard Python strftime directives\n",
    "    - %d: Day of the month as a zero-padded decimal (e.g., 25)\n",
    "    - %m: Month as a zero-padded decimal number (e.g., 08)\n",
    "    - %b: Abbreviated month name (e.g., Aug)\n",
    "    - %Y: Four-digit year (e.g., 2024)\n",
    "- Smart case handling: Automatically normalizes month abbreviations (like 'JAN', 'jan', 'Jan') when using %b format\n",
    "- Type safety: Preserves existing datetime columns without unnecessary conversion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date1         object\n",
      "date2         object\n",
      "date3         object\n",
      "date4         object\n",
      "not_a_date    object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Sample DataFrame with different date formats\n",
    "df_test = pd.DataFrame({\n",
    "    \"date1\": [\"20240101\", \"20240215\", \"20240320\"],\n",
    "    \"date2\": [\"25-08-2024\", \"15-09-2024\", \"01-10-2024\"],\n",
    "    \"date3\": [\"25Aug2024\", \"15AUG2024\", \"01aug2024\"],  # different cases\n",
    "    \"date4\": [\"20240801\", \"20240915\", \"20240311\"],\n",
    "    \"not_a_date\": [123, \"abc\", None]\n",
    "})\n",
    "print(df_test.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date1         datetime64[ns]\n",
      "date2         datetime64[ns]\n",
      "date3         datetime64[ns]\n",
      "date4         datetime64[ns]\n",
      "not_a_date            object\n",
      "dtype: object\n",
      "       date1      date2      date3      date4 not_a_date\n",
      "0 2024-01-01 2024-08-25 2024-08-25 2024-08-01        123\n",
      "1 2024-02-15 2024-09-15 2024-08-15 2024-09-15        abc\n",
      "2 2024-03-20 2024-10-01 2024-08-01 2024-03-11       None\n"
     ]
    }
   ],
   "source": [
    "# Apply the function \n",
    "df_result = dut.transform_date_cols(df_test, [\"date1\", \"date4\"], \"%Y%m%d\") # take a list\n",
    "df_result = dut.transform_date_cols(df_result, \"date2\", \"%d-%m-%Y\") # take one column\n",
    "df_result = dut.transform_date_cols(df_result, [\"date3\"], \"%d%b%Y\") # handle column with different cases automatically\n",
    "\n",
    "# Display result\n",
    "print(df_result.dtypes)\n",
    "print(df_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Clean Dollar Columns\n",
    "It is common for a dataframe to have dollar amount columns stored as strings with currency symbols and commas. The `clean_dollar_cols` function helps you transform these into numeric values.\n",
    "\n",
    "- Flexible input handling: Works with either a single column or multiple columns\n",
    "- Clean the column(s) off currency symbols and commas\n",
    "- Type conversion: Converts the cleaned strings to float values for numerical analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "       price  revenue other\n",
      "0  $1,234.56   12,000     A\n",
      "1  $2,345.67              B\n",
      "2  $3,456.78  $30,000     C\n",
      "\n",
      "Dtypes:\n",
      "price      object\n",
      "revenue    object\n",
      "other      object\n",
      "dtype: object\n",
      "\n",
      "Cleaned DataFrame:\n",
      "     price  revenue other\n",
      "0  1234.56  12000.0     A\n",
      "1  2345.67      NaN     B\n",
      "2  3456.78  30000.0     C\n",
      "\n",
      "Dtypes:\n",
      "price      float64\n",
      "revenue    float64\n",
      "other       object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "df_dollars = pd.DataFrame({\n",
    "    'price': ['$1,234.56', '$2,345.67', '$3,456.78'],\n",
    "    'revenue': ['12,000', '', '$30,000'],\n",
    "    'other': ['A', 'B', 'C']\n",
    "})\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "print(df_dollars)\n",
    "print(\"\\nDtypes:\")\n",
    "print(df_dollars.dtypes)\n",
    "\n",
    "df_cleaned = dut.clean_dollar_cols(df_dollars, ['price', 'revenue'])\n",
    "\n",
    "print(\"\\nCleaned DataFrame:\")\n",
    "print(df_cleaned)\n",
    "print(\"\\nDtypes:\")\n",
    "print(df_cleaned.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Value Counts with Percent\n",
    "The `value_counts_with_pct` function enhances pandas' built-in value_counts by adding percentage information alongside counts.\n",
    "\n",
    "- Comprehensive view: Shows both raw counts and percentages in a single output\n",
    "- Flexible NA handling: Option to include or exclude NA values from the analysis\n",
    "- Clear formatting: Percentages are formatted with a specified number of decimal places\n",
    "- Sorted results: Values are sorted by frequency for easy interpretation\n",
    "- Useful for: Quick categorical data profiling, understanding class distributions, and reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value counts for 'color' (including NA):\n",
      "   color  count    pct\n",
      "0    red      3  42.86\n",
      "1   blue      2  28.57\n",
      "2  green      1  14.29\n",
      "3   None      1  14.29\n",
      "\n",
      "Value counts for 'color' (excluding NA):\n",
      "   color  count    pct\n",
      "0    red      3  50.00\n",
      "1   blue      2  33.33\n",
      "2  green      1  16.67\n",
      "\n",
      "Value counts for ['color', 'size']:\n",
      "   color size  count    pct\n",
      "0    red    L      2  28.57\n",
      "1   blue    M      1  14.29\n",
      "2   blue    S      1  14.29\n",
      "3  green    M      1  14.29\n",
      "4    red    S      1  14.29\n",
      "5    NaN    M      1  14.29\n"
     ]
    }
   ],
   "source": [
    "df_categories = pd.DataFrame({\n",
    "    'color': ['red', 'blue', 'red', 'green', 'blue', 'red', None],\n",
    "    'size': ['S', 'M', 'L', 'M', 'S', 'L', 'M']\n",
    "})\n",
    "\n",
    "print(\"Value counts for 'color' (including NA):\")\n",
    "print(dut.value_counts_with_pct(df_categories, 'color'))\n",
    "\n",
    "print(\"\\nValue counts for 'color' (excluding NA):\")\n",
    "print(dut.value_counts_with_pct(df_categories, 'color', dropna=True))\n",
    "\n",
    "print(\"\\nValue counts for ['color', 'size']:\")\n",
    "print(dut.value_counts_with_pct(df_categories, ['color','size']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Drop Fully Null Columns\n",
    "The `drop_fully_null_cols` function is specifically designed to prevent issues with Databricks' `display()` function, which can break when encountering columns that are entirely null (as it cannot infer the schema).\n",
    "\n",
    "- Prevents Databricks display errors: Removes columns that would cause schema inference issues\n",
    "- Safe operation: Returns a new DataFrame without modifying the original\n",
    "- Common usage: `drop_fully_null_cols(df).display()` in Databricks notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "   col1  col2  col3  col4\n",
      "0     1   NaN     A  None\n",
      "1     2   NaN  None  None\n",
      "2     3   NaN     C  None\n",
      "üóëÔ∏è Dropped fully-null columns: ['col2', 'col4']\n",
      "\n",
      "Cleaned DataFrame:\n",
      "   col1  col3\n",
      "0     1     A\n",
      "1     2  None\n",
      "2     3     C\n"
     ]
    }
   ],
   "source": [
    "df_nulls = pd.DataFrame({\n",
    "    'col1': [1, 2, 3],\n",
    "    'col2': [np.nan, np.nan, np.nan],  # Fully null\n",
    "    'col3': ['A', None, 'C'],\n",
    "    'col4': [None, None, None]  # Fully null\n",
    "})\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "print(df_nulls)\n",
    "\n",
    "df_cleaned = dut.drop_fully_null_cols(df_nulls, verbose=True)\n",
    "\n",
    "print(\"\\nCleaned DataFrame:\")\n",
    "print(df_cleaned) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Print Schema Alphabetically\n",
    "The `print_schema_alphabetically` function is particularly useful when exploring very wide DataFrames with many columns. By sorting column names alphabetically, it makes it easier to:\n",
    "\n",
    "- Quickly locate specific columns in large datasets\n",
    "- Compare schemas between different DataFrames to identify missing or additional columns\n",
    "- Maintain a consistent view of your data structure regardless of the original column order\n",
    "- Simplify documentation and reporting of data structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "   z_price a_category     m_date  b_is_active  y_quantity\n",
      "0    100.5          A 2024-01-01         True           1\n",
      "1    200.5          B 2024-01-02        False           2\n",
      "2    300.5          C 2024-01-03         True           3\n",
      "\n",
      "Schema in alphabetical order:\n",
      "a_category             object\n",
      "b_is_active              bool\n",
      "m_date         datetime64[ns]\n",
      "y_quantity              int32\n",
      "z_price               float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "    'z_price': [100.5, 200.5, 300.5],\n",
    "    'a_category': ['A', 'B', 'C'],\n",
    "    'm_date': pd.date_range('2024-01-01', periods=3),\n",
    "    'b_is_active': [True, False, True],\n",
    "    'y_quantity': np.array([1, 2, 3], dtype='int32')\n",
    "})\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "print(\"\\nSchema in alphabetical order:\")\n",
    "dut.print_schema_alphabetically(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Check Primary Key\n",
    "The `is_primary_key` function helps verify if a column or combination of columns could serve as a primary key in a DataFrame.\n",
    "\n",
    "A traditional primary key must satisfy two key requirements:\n",
    "1. Uniqueness: Each combination of values must be unique across all rows\n",
    "2. No null values: Primary key columns cannot contain null/missing values\n",
    "\n",
    "However, in real-world data analysis, we often encounter datasets where potential key columns contain some missing values. This function takes a practical approach by:\n",
    "1. Alerting you about any missing values in the potential key columns\n",
    "2. Checking if the columns would form a unique identifier after removing rows with missing values\n",
    "\n",
    "This function is useful for:\n",
    "- Data quality assessment: Understanding the completeness and uniqueness of your key fields\n",
    "- Database schema design: Identifying potential primary keys even in imperfect data\n",
    "- ETL validation: Verifying key constraints while being aware of data quality issues\n",
    "- Data integrity checks: Ensuring uniqueness for joins/merges after handling missing values\n",
    "\n",
    "The function accepts either a single column name or a list of columns, making it flexible for checking both simple and composite keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test 1: Single column primary key\n",
      "‚úÖ There are no missing values in column 'id'.\n",
      "‚ÑπÔ∏è Total row count after filtering out missings: 5\n",
      "‚ÑπÔ∏è Unique row count after filtering out missings: 5\n",
      "üîë The column(s) 'id' form a primary key.\n",
      "\n",
      "Test 2: Column with duplicates\n",
      "‚úÖ There are no missing values in column 'category'.\n",
      "‚ÑπÔ∏è Total row count after filtering out missings: 5\n",
      "‚ÑπÔ∏è Unique row count after filtering out missings: 3\n",
      "‚ùå The column(s) 'category' do not form a primary key.\n",
      "\n",
      "Test 3: Composite primary key\n",
      "‚úÖ There are no missing values in columns 'category', 'date'.\n",
      "‚ÑπÔ∏è Total row count after filtering out missings: 5\n",
      "‚ÑπÔ∏è Unique row count after filtering out missings: 5\n",
      "üîë The column(s) 'category', 'date' form a primary key.\n",
      "\n",
      "Test 4: Column with null values\n",
      "‚ö†Ô∏è There are 1 row(s) with missing values in column 'code'.\n",
      "‚úÖ There are no missing values in column 'date'.\n",
      "‚ÑπÔ∏è Total row count after filtering out missings: 4\n",
      "‚ÑπÔ∏è Unique row count after filtering out missings: 4\n",
      "üîë The column(s) 'code', 'date' form a primary key after removing rows with missing values.\n",
      "\n",
      "Test 5: Empty DataFrame\n",
      "‚ùå DataFrame is empty.\n",
      "\n",
      "Test 6: Non-existent column\n",
      "‚ùå Column(s) 'not_a_column' do not exist in the DataFrame.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create sample DataFrame with different primary key scenarios\n",
    "df = pd.DataFrame({\n",
    "    # Single column primary key\n",
    "    'id': [1, 2, 3, 4, 5],\n",
    "    \n",
    "    # Column with duplicates\n",
    "    'category': ['A', 'B', 'A', 'B', 'C'],\n",
    "    \n",
    "    # Date column with some duplicates\n",
    "    'date': ['2024-01-01', '2024-01-01', '2024-01-02', '2024-01-02', '2024-01-03'],\n",
    "    \n",
    "    # Column with null values\n",
    "    'code': ['X1', None, 'X3', 'X4', 'X5'],\n",
    "    \n",
    "    # Values column\n",
    "    'value': [100, 200, 300, 400, 500]\n",
    "})\n",
    "\n",
    "# Test 1: Single column that is a primary key\n",
    "print(\"\\nTest 1: Single column primary key\")\n",
    "dut.is_primary_key(df, ['id'])  # Should return True\n",
    "\n",
    "# Test 2: Single column that is not a primary key (has duplicates)\n",
    "print(\"\\nTest 2: Column with duplicates\")\n",
    "dut.is_primary_key(df, ['category'])  # Should return False\n",
    "\n",
    "# Test 3: Multiple columns that together form a primary key\n",
    "print(\"\\nTest 3: Composite primary key\")\n",
    "dut.is_primary_key(df, ['category', 'date'])  # Should return True\n",
    "\n",
    "# Test 4: Column with null values\n",
    "print(\"\\nTest 4: Column with null values\")\n",
    "dut.is_primary_key(df, ['code','date'])  # Should return False\n",
    "\n",
    "# Test 5: Empty DataFrame\n",
    "print(\"\\nTest 5: Empty DataFrame\")\n",
    "empty_df = pd.DataFrame(columns=['id', 'value'])\n",
    "dut.is_primary_key(empty_df, ['id'])  # Should return False\n",
    "\n",
    "# Test 6: Non-existent column\n",
    "print(\"\\nTest 6: Non-existent column\")\n",
    "dut.is_primary_key(df, ['not_a_column'])  # Should return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Select Existing Columns\n",
    "The `select_existing_cols` function provides a safe way to select columns from a DataFrame, handling cases where some requested columns might not exist.\n",
    "\n",
    "- Safe column selection: Returns only columns that exist in the DataFrame\n",
    "- Case sensitivity options: Can match column names exactly or case-insensitively\n",
    "- Verbose mode: Optional detailed output about which columns were found/missing\n",
    "- Useful for: Data pipeline robustness, handling dynamic column selections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "   A  B  C  Mixed_Case\n",
      "0  1  4  7          10\n",
      "1  2  5  8          11\n",
      "2  3  6  9          12\n",
      "\n",
      "Example 1: Select existing columns\n",
      "Selected 'A', 'C', 'D' (D doesn't exist):\n",
      "   A  C\n",
      "0  1  7\n",
      "1  2  8\n",
      "2  3  9\n",
      "\n",
      "Example 2: Case-insensitive matching\n",
      "‚úÖ Columns found: ['A', 'Mixed_Case']\n",
      "   A  Mixed_Case\n",
      "0  1          10\n",
      "1  2          11\n",
      "2  3          12\n",
      "\n",
      "Example 3: Verbose output with missing columns\n",
      "‚úÖ Columns found: ['A', 'B']\n",
      "‚ö†Ô∏è Columns not found: ['Missing1', 'Missing2']\n",
      "   A  B\n",
      "0  1  4\n",
      "1  2  5\n",
      "2  3  6\n"
     ]
    }
   ],
   "source": [
    "# Create sample DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'A': [1, 2, 3],\n",
    "    'B': [4, 5, 6],\n",
    "    'C': [7, 8, 9],\n",
    "    'Mixed_Case': [10, 11, 12]\n",
    "})\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# Example 1: Basic usage\n",
    "print(\"\\nExample 1: Select existing columns\")\n",
    "result1 = dut.select_existing_cols(df, ['A', 'C', 'D'])\n",
    "print(\"Selected 'A', 'C', 'D' (D doesn't exist):\")\n",
    "print(result1)\n",
    "\n",
    "# Example 2: Case-insensitive matching\n",
    "print(\"\\nExample 2: Case-insensitive matching\")\n",
    "result2 = dut.select_existing_cols(df, ['a', 'mixed_case'], strict=False, verbose=True)\n",
    "print(result2)\n",
    "\n",
    "# Example 3: Verbose output\n",
    "print(\"\\nExample 3: Verbose output with missing columns\")\n",
    "result3 = dut.select_existing_cols(df, ['A', 'Missing1', 'B', 'Missing2'], verbose=True)\n",
    "print(result3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".test_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
